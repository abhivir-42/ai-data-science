# Understanding the make_data_cleaning_agent Function

The `make_data_cleaning_agent` function is the core of the data cleaning agent. It creates a LangGraph state machine that defines the workflow for the data cleaning process. This function is typically not called directly by users, but is instead called by the `DataCleaningAgent` class's `_make_compiled_graph` method.

## Function Signature

```python
def make_data_cleaning_agent(
    model, 
    n_samples = 30, 
    log=False, 
    log_path=None, 
    file_name="data_cleaner.py",
    function_name="data_cleaner",
    overwrite = True, 
    human_in_the_loop=False, 
    bypass_recommended_steps=False, 
    bypass_explain_code=False,
    checkpointer: Checkpointer = None
):
    # Implementation details...
```

The function parameters are identical to those of the `DataCleaningAgent` class's `__init__` method, as they are simply passed through. Let's review them:

- **model**: The language model (e.g., from OpenAI or Anthropic) that will power the agent.
- **n_samples**: The number of sample rows to use when summarizing the dataset for the model.
- **log**: Whether to log generated code to a file.
- **log_path**: The directory where log files should be stored.
- **file_name**: The name of the file to save the generated code to.
- **function_name**: The name of the Python function that will be generated.
- **overwrite**: Whether to overwrite existing log files.
- **human_in_the_loop**: Whether to enable human review of recommendations.
- **bypass_recommended_steps**: Whether to skip the recommendation generation step.
- **bypass_explain_code**: Whether to skip the code explanation step.
- **checkpointer**: An optional checkpointer for saving and loading the agent's state.

## Setting Up the Checkpointer

```python
if human_in_the_loop:
    if checkpointer is None:
        print("Human in the loop is enabled. A checkpointer is required. Setting to MemorySaver().")
        checkpointer = MemorySaver()
```

If human-in-the-loop functionality is enabled, a checkpointer is required to save the state of the workflow before the human review step. If no checkpointer is provided, a `MemorySaver` is created to save the state in memory.

## Parameter Validation and Adjustments

```python
# Human in the loop requires recommended steps
if bypass_recommended_steps and human_in_the_loop:
    bypass_recommended_steps = False
    print("Bypass recommended steps set to False to enable human in the loop.")
```

This code ensures that if human-in-the-loop functionality is enabled, the recommendation generation step cannot be bypassed. This is because the human review step needs the recommendations to review.

## Setting Up Logging

```python
# Setup Log Directory
if log:
    if log_path is None:
        log_path = LOG_PATH
    if not os.path.exists(log_path):
        os.makedirs(log_path)
```

If logging is enabled, this code ensures that the log directory exists, creating it if necessary. If no log path is provided, the default `LOG_PATH` is used.

## Defining the Graph State

```python
# Define GraphState for the router
class GraphState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], operator.add]
    user_instructions: str
    recommended_steps: str
    data_raw: dict
    data_cleaned: dict
    all_datasets_summary: str
    data_cleaner_function: str
    data_cleaner_function_path: str
    data_cleaner_file_name: str
    data_cleaner_function_name: str
    data_cleaner_error: str
    max_retries: int
    retry_count: int
```

This defines the structure of the state that will be shared among all nodes in the LangGraph workflow. The state includes:

- **messages**: A list of messages (with an `operator.add` annotation to indicate that new messages should be appended to the list).
- **user_instructions**: The user's instructions for the data cleaning process.
- **recommended_steps**: The recommended cleaning steps generated by the agent.
- **data_raw**: The raw data, initially in a pandas DataFrame format, is converted to a dictionary to facilitate flexible and dynamic handling during the data cleaning process. This conversion enables easier manipulation and analysis of the data structure, allowing the agent to efficiently identify and address data cleaning tasks.
- **data_cleaned**: The cleaned data as a dictionary.
- **all_datasets_summary**: A summary of all datasets provided.
- **data_cleaner_function**: The generated data cleaning function as a string.
- **data_cleaner_function_path**: The path to the log file where the function was saved.
- **data_cleaner_file_name**: The name of the log file.
- **data_cleaner_function_name**: The name of the generated function.
- **data_cleaner_error**: Any error that occurred during function execution.
- **max_retries**: The maximum number of times to retry if errors occur.
- **retry_count**: The current retry count.

The `TypedDict` class is employed to explicitly define the structure of a dictionary, ensuring that it adheres to a specific format. In this context, it is used to define the `GraphState` dictionary with its expected keys and their corresponding types. Additionally, the `Annotated` class is utilized to provide supplementary information about the type of the `messages` field within the `GraphState` dictionary. This annotation indicates that the `messages` field is expected to be a sequence of `BaseMessage` objects, and it should support the `operator.add` operation for appending new messages.

## Node Functions

The next section of the function defines several node functions that will be used in the LangGraph workflow. Each node function takes a `GraphState` object as input and returns a dictionary with updates to apply to the state.

### Recommend Cleaning Steps

```python
def recommend_cleaning_steps(state: GraphState):
    """
    Recommend a series of data cleaning steps based on the input data. 
    These recommended steps will be appended to the user_instructions.
    """
    print(format_agent_name(AGENT_NAME))
    print("    * RECOMMEND CLEANING STEPS")

    # Prompt to get recommended steps from the LLM
    recommend_steps_prompt = PromptTemplate(
        template="""
        You are a Data Cleaning Expert. Given the following information about the data, 
        recommend a series of numbered steps to take to clean and preprocess it. 
        The steps should be tailored to the data characteristics and should be helpful 
        for a data cleaning agent that will be implemented.
        
        General Steps:
        Things that should be considered in the data cleaning steps:
        
        * Removing columns if more than 40 percent of the data is missing
        * Imputing missing values with the mean of the column if the column is numeric
        * Imputing missing values with the mode of the column if the column is categorical
        * Converting columns to the correct data type
        * Removing duplicate rows
        * Removing rows with missing values
        * Removing rows with extreme outliers (3X the interquartile range)
        
        Custom Steps:
        * Analyze the data to determine if any additional data cleaning steps are needed.
        * Recommend steps that are specific to the data provided. Include why these steps are necessary or beneficial.
        * If no additional steps are needed, simply state that no additional steps are required.
        
        IMPORTANT:
        Make sure to take into account any additional user instructions that may add, remove or modify some of these steps. Include comments in your code to explain your reasoning for each step. Include comments if something is not done because a user requested. Include comments if something is done because a user requested.
        
        User instructions:
        {user_instructions}

        Previously Recommended Steps (if any):
        {recommended_steps}

        Below are summaries of all datasets provided:
        {all_datasets_summary}

        Return steps as a numbered list. You can return short code snippets to demonstrate actions. But do not return a fully coded solution. The code will be generated separately by a Coding Agent.
        
        Avoid these:
        1. Do not include steps to save files.
        2. Do not include unrelated user instructions that are not related to the data cleaning.
        """,
        input_variables=["user_instructions", "recommended_steps", "all_datasets_summary"]
    )

    data_raw = state.get("data_raw")
    df = pd.DataFrame.from_dict(data_raw)

    all_datasets_summary = get_dataframe_summary([df], n_sample=n_samples) 
    # get_dataframe_summary is defined in src.tools.dataframe.py
    
    all_datasets_summary_str = "\n\n".join(all_datasets_summary)

    steps_agent = recommend_steps_prompt | llm
    recommended_steps = steps_agent.invoke({
        "user_instructions": state.get("user_instructions"),
        "recommended_steps": state.get("recommended_steps"),
        "all_datasets_summary": all_datasets_summary_str
    }) 
    
    return {
        "recommended_steps": format_recommended_steps(recommended_steps.content.strip(), heading="# Recommended Data Cleaning Steps:"),
        "all_datasets_summary": all_datasets_summary_str
    }
```

This node function generates a list of recommended data cleaning steps based on the input data and user instructions. It:

1. Creates a `PromptTemplate` with instructions for the language model.
2. Converts the raw data dictionary to a pandas DataFrame.
3. Generates a summary of the DataFrame using the `get_dataframe_summary` function.
4. Creates a "steps agent" by piping the prompt template to the language model.
5. Invokes the steps agent with the user instructions, any previously recommended steps, and the dataset summary.
6. Returns a dictionary with the recommended steps (formatted with a heading) and the dataset summary.

The prompt template is quite detailed, providing guidance to the language model on what kinds of cleaning steps to recommend and how to present them. It also emphasizes the importance of considering the user's instructions and the specific characteristics of the data.

### Create Data Cleaner Code

```python
def create_data_cleaner_code(state: GraphState):
    
    print("    * CREATE DATA CLEANER CODE")
    
    if bypass_recommended_steps:
        print(format_agent_name(AGENT_NAME))
        
        data_raw = state.get("data_raw")
        df = pd.DataFrame.from_dict(data_raw)

        all_datasets_summary = get_dataframe_summary([df], n_sample=n_samples)
        
        all_datasets_summary_str = "\n\n".join(all_datasets_summary)
    else:
        all_datasets_summary_str = state.get("all_datasets_summary")
    
    
    data_cleaning_prompt = PromptTemplate(
        template="""
        You are a Data Cleaning Agent. Your job is to create a {function_name}() function that can be run on the data provided using the following recommended steps.

        Recommended Steps:
        {recommended_steps}

        You can use Pandas, Numpy, and Scikit Learn libraries to clean the data.

        Below are summaries of all datasets provided. Use this information about the data to help determine how to clean the data:

        {all_datasets_summary}

        Return Python code in ```python``` format with a single function definition, {function_name}(data_raw), that includes all imports inside the function.

        Return code to provide the data cleaning function:

        def {function_name}(data_raw):
            import pandas as pd
            import numpy as np
            ...
            return data_cleaned

        Best Practices and Error Preventions:

        Always ensure that when assigning the output of fit_transform() from SimpleImputer to a Pandas DataFrame column, you call .ravel() or flatten the array, because fit_transform() returns a 2D array while a DataFrame column is 1D.
        
        """,
        input_variables=["recommended_steps", "all_datasets_summary", "function_name"]
    )

    data_cleaning_agent = data_cleaning_prompt | llm | PythonOutputParser()
    
    response = data_cleaning_agent.invoke({
        "recommended_steps": state.get("recommended_steps"),
        "all_datasets_summary": all_datasets_summary_str,
        "function_name": function_name
    })
    
    response = relocate_imports_inside_function(response)
    response = add_comments_to_top(response, agent_name=AGENT_NAME)
    
    # For logging: store the code generated:
    file_path, file_name_2 = log_ai_function(
        response=response,
        file_name=file_name,
        log=log,
        log_path=log_path,
        overwrite=overwrite
    )
   
    return {
        "data_cleaner_function" : response,
        "data_cleaner_function_path": file_path,
        "data_cleaner_file_name": file_name_2,
        "data_cleaner_function_name": function_name,
        "all_datasets_summary": all_datasets_summary_str
    }
```

This node function generates Python code to implement the recommended data cleaning steps. It:

1. If `bypass_recommended_steps` is `True`, generates a dataset summary; otherwise, uses the one from the state.
2. Creates a `PromptTemplate` with instructions for the language model.
3. Creates a "data cleaning agent" by piping the prompt template to the language model and then to a `PythonOutputParser`.
4. Invokes the data cleaning agent with the recommended steps, dataset summary, and function name.
5. Processes the response using helper functions to relocate imports inside the function and add comments to the top.
6. Logs the function to a file if logging is enabled.
7. Returns a dictionary with the generated function, its path, name, and the dataset summary.

The `PythonOutputParser` is used to extract clean Python code from the language model's response, which might include markdown formatting.

### Human Review

```python
prompt_text_human_review = "Are the following data cleaning instructions correct? (Answer 'yes' or provide modifications)\n{steps}"

if not bypass_explain_code:
    def human_review(state: GraphState) -> Command[Literal["recommend_cleaning_steps", "explain_data_cleaner_code"]]:
        return node_func_human_review(
            state=state,
            prompt_text=prompt_text_human_review,
            yes_goto= 'explain_data_cleaner_code',
            no_goto="recommend_cleaning_steps",
            user_instructions_key="user_instructions",
            recommended_steps_key="recommended_steps",
            code_snippet_key="data_cleaner_function",
        )
else:
    def human_review(state: GraphState) -> Command[Literal["recommend_cleaning_steps", "__end__"]]:
        return node_func_human_review(
            state=state,
            prompt_text=prompt_text_human_review,
            yes_goto= '__end__',
            no_goto="recommend_cleaning_steps",
            user_instructions_key="user_instructions",
            recommended_steps_key="recommended_steps",
            code_snippet_key="data_cleaner_function", 
        )
```

This code defines a human review node function that allows a human to review the recommended cleaning steps. It uses the `node_func_human_review` helper function, which:

1. Presents the recommended steps to the human.
2. If the human says "yes", continues to the next step (`explain_data_cleaner_code` or `__end__` depending on `bypass_explain_code`).
3. If the human says anything else, goes back to the recommendation step with the human's response as additional user instructions.

The `Command` return type and `Literal` type hint are used to specify the possible next nodes in the workflow.

### Execute Data Cleaner Code

```python
def execute_data_cleaner_code(state):
    return node_func_execute_agent_code_on_data(
        state=state,
        data_key="data_raw",
        result_key="data_cleaned",
        error_key="data_cleaner_error",
        code_snippet_key="data_cleaner_function",
        agent_function_name=state.get("data_cleaner_function_name"),
        pre_processing=lambda data: pd.DataFrame.from_dict(data),
        post_processing=lambda df: df.to_dict() if isinstance(df, pd.DataFrame) else df,
        error_message_prefix="An error occurred during data cleaning: "
    )
```

This node function executes the generated data cleaning function on the raw data. It uses the `node_func_execute_agent_code_on_data` helper function, which:

1. Gets the raw data and code from the state.
2. Preprocesses the data (converting it from a dictionary to a pandas DataFrame).
3. Executes the code to define the cleaning function.
4. Calls the cleaning function on the preprocessed data.
5. Postprocesses the result (converting it from a pandas DataFrame to a dictionary).
6. Returns the result and any error that occurred.

### Fix Data Cleaner Code

```python
def fix_data_cleaner_code(state: GraphState):
    data_cleaner_prompt = """
    You are a Data Cleaning Agent. Your job is to create a {function_name}() function that can be run on the data provided. The function is currently broken and needs to be fixed.
    
    Make sure to only return the function definition for {function_name}().
    
    Return Python code in ```python``` format with a single function definition, {function_name}(data_raw), that includes all imports inside the function.
    
    This is the broken code (please fix): 
    {code_snippet}

    Last Known Error:
    {error}
    """

    return node_func_fix_agent_code(
        state=state,
        code_snippet_key="data_cleaner_function",
        error_key="data_cleaner_error",
        llm=llm,  
        prompt_template=data_cleaner_prompt,
        agent_name=AGENT_NAME,
        log=log,
        file_path=state.get("data_cleaner_function_path"),
        function_name=state.get("data_cleaner_function_name"),
    )
```

This node function fixes the data cleaning function if an error occurred during execution. It uses the `node_func_fix_agent_code` helper function, which:

1. Gets the broken code and error from the state.
2. Creates a prompt with the broken code and error.
3. Invokes the language model with the prompt to get a fixed version of the code.
4. Processes the response and logs it if logging is enabled.
5. Returns the fixed code.

### Report Agent Outputs

```python
def report_agent_outputs(state: GraphState):
    return node_func_report_agent_outputs(
        state=state,
        keys_to_include=[
            "recommended_steps",
            "data_cleaner_function",
            "data_cleaner_function_path",
            "data_cleaner_function_name",
            "data_cleaner_error",
        ],
        result_key="messages",
        role=AGENT_NAME,
        custom_title="Data Cleaning Agent Outputs"
    )
```

This node function creates a report of the agent's outputs. It uses the `node_func_report_agent_outputs` helper function, which:

1. Gets the specified keys from the state.
2. Creates a JSON report with those keys.
3. Creates a message with the report.
4. Adds the message to the `messages` list in the state.

## Creating the LangGraph State Machine

The final section of the function creates a LangGraph state machine using the node functions defined above.

```python
node_functions = {
    "recommend_cleaning_steps": recommend_cleaning_steps,
    "human_review": human_review,
    "create_data_cleaner_code": create_data_cleaner_code,
    "execute_data_cleaner_code": execute_data_cleaner_code,
    "fix_data_cleaner_code": fix_data_cleaner_code,
    "report_agent_outputs": report_agent_outputs, 
}

app = create_coding_agent_graph(
    GraphState=GraphState,
    node_functions=node_functions,
    recommended_steps_node_name="recommend_cleaning_steps",
    create_code_node_name="create_data_cleaner_code",
    execute_code_node_name="execute_data_cleaner_code",
    fix_code_node_name="fix_data_cleaner_code",
    explain_code_node_name="report_agent_outputs", 
    error_key="data_cleaner_error",
    human_in_the_loop=human_in_the_loop,
    human_review_node_name="human_review",
    checkpointer=checkpointer,
    bypass_recommended_steps=bypass_recommended_steps,
    bypass_explain_code=bypass_explain_code,
    agent_name=AGENT_NAME,
)

return app
```

This code:

1. Creates a dictionary mapping node names to node functions.
2. Calls the `create_coding_agent_graph` helper function to create a LangGraph state machine with the specified nodes, edges, and parameters.
3. Returns the compiled state machine.

The `create_coding_agent_graph` function abstracts the complexity of creating a LangGraph state machine with the appropriate nodes and edges for a coding agent. This makes it easier to create different types of coding agents with similar workflows but different node functions.

## Summary

The `make_data_cleaning_agent` function creates a LangGraph state machine that defines the workflow for the data cleaning process. It:

1. Defines the structure of the state that will be shared among all nodes.
2. Defines node functions for each step in the workflow.
3. Creates a LangGraph state machine with the appropriate nodes and edges.
4. Returns the compiled state machine.

The function is designed to be flexible and configurable, allowing users to customize the data cleaning process through parameters like `human_in_the_loop`, `bypass_recommended_steps`, and `bypass_explain_code`.
