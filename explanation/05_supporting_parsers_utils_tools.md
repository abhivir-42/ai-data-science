# Understanding Supporting Parsers, Utils, and Tools

The data cleaning agent relies on several supporting modules for parsing, utility functions, and tools. These modules provide functionality that is used throughout the agent's workflow. In this section, we'll explore the key supporting modules and how they're used by the data cleaning agent.

## Parsers

The `parsers.py` module provides parsers for standardizing output from language models. The data cleaning agent uses the `PythonOutputParser` class to extract clean Python code from the language model's response.

### PythonOutputParser

```python
class PythonOutputParser(BaseOutputParser):
    def parse(self, text: str):        
        def extract_python_code(text):
            python_code_match = re.search(r'```python(.*?)```', text, re.DOTALL)
            if python_code_match:
                python_code = python_code_match.group(1).strip()
                return python_code
            else:
                python_code_match = re.search(r"python(.*?)'", text, re.DOTALL)
                if python_code_match:
                    python_code = python_code_match.group(1).strip()
                    return python_code
                else:
                    return None
        python_code = extract_python_code(text)
        if python_code is not None:
            return python_code
        else:
            # Assume ```python wasn't used
            return text
```

This class:

1. Inherits from `BaseOutputParser` from LangChain.
2. Implements a `parse` method that extracts Python code from text.
3. Uses regular expressions to find code blocks enclosed in triple backticks with "python" language specifier.
4. Returns the extracted code if found, or the original text if no code block is found.

This parser is used in the data cleaning agent to extract Python code from the language model's response when generating the data cleaning function and when fixing broken code.

## Utils

The data cleaning agent uses several utility modules for various tasks:

### regex.py

The `regex.py` module provides utility functions for working with text using regular expressions. The data cleaning agent uses the following functions from this module:

#### relocate_imports_inside_function

```python
def relocate_imports_inside_function(code_text):
    """
    Relocates all import statements in a given Python function and moves them inside the function definition.
    """
    # Match all import statements
    import_pattern = r'^\s*(import\s+[^\n]+|from\s+\S+\s+import\s+[^\n]+)\s*$'
    imports = re.findall(import_pattern, code_text, re.MULTILINE)

    # Remove imports from the top-level code
    code_without_imports = re.sub(import_pattern, '', code_text, flags=re.MULTILINE).strip()

    # Find the function definition and insert the imports inside it
    function_pattern = r'(def\s+\w+\s*\(.*?\):)'
    match = re.search(function_pattern, code_without_imports)

    if match:
        function_start = match.end()
        # Insert the imports right after the function definition
        imports_code = '\n    ' + '\n    '.join(imports)  # Indent imports
        modified_code = (
            code_without_imports[:function_start]
            + imports_code
            + code_without_imports[function_start:]
        )
        return modified_code

    # If no function is found, return the original code
    return code_text
```

This function:

1. Finds all import statements in the code using a regular expression.
2. Removes the import statements from the code.
3. Finds the function definition using a regular expression.
4. Inserts the import statements inside the function, right after the function definition line.
5. Returns the modified code.

This function is used to ensure that all imports are inside the generated data cleaning function, which is a best practice for functions that will be executed in a dynamic context.

#### add_comments_to_top

```python
def add_comments_to_top(code_text, agent_name="data_wrangler"):
    """
    Adds AI-generated metadata comments to the top of the Python code.
    """
    # Generate timestamp
    time_created = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    # Construct the header comments
    header_comments = [
        "# Disclaimer: This function was generated by AI. Please review before using.",
        f"# Agent Name: {agent_name}",
        f"# Time Created: {time_created}\n",        
        ""
    ]

    # Join the header with newlines, then prepend to the existing code_text
    header_block = "\n".join(header_comments)
    return header_block + code_text
```

This function:

1. Generates a timestamp for the current time.
2. Constructs header comments with a disclaimer, agent name, and timestamp.
3. Adds the header comments to the top of the code.

This function is used to add metadata comments to the generated data cleaning function, making it clear that the function was generated by AI and when it was created.

#### format_agent_name

```python
def format_agent_name(agent_name: str) -> str:
    
    formatted_name = agent_name.strip().replace("_", " ").upper()
    
    return f"---{formatted_name}----"
```

This function:

1. Strips leading and trailing whitespace from the agent name.
2. Replaces underscores with spaces.
3. Converts the name to uppercase.
4. Surrounds the name with hyphens.

This function is used for formatting the agent name in log messages and output reports.

#### format_recommended_steps

```python
def format_recommended_steps(raw_text: str, heading: str = "# Recommended Steps:") -> str:
    # Split text by newline and strip leading/trailing whitespace
    lines = raw_text.strip().split('\n')
    
    # Remove empty lines from the start
    while lines and not lines[0].strip():
        lines.pop(0)

    seen_heading = False
    new_lines = []

    for line in lines:
        # If this line *is exactly* the heading, check if we've seen it already
        if line.strip() == heading:
            if seen_heading:
                # Skip duplicates
                continue
            else:
                seen_heading = True
        new_lines.append(line)

    # If heading was never seen, prepend it
    if not seen_heading:
        new_lines.insert(0, heading)

    return "\n".join(new_lines)
```

This function:

1. Splits the text into lines and strips leading/trailing whitespace.
2. Removes empty lines from the start.
3. Checks if the heading already exists in the text; if not, adds it at the beginning.
4. Removes duplicate headings.
5. Joins the lines back together.

This function is used to format the recommended cleaning steps generated by the language model, ensuring that they have a consistent heading.

#### get_generic_summary

```python
def get_generic_summary(report_dict: dict, code_lang = "python") -> str:
    """
    Takes a dictionary of unknown structure (e.g., from json.loads(...)) 
    and returns a textual summary. It assumes:
      1) 'report_title' (if present) should be displayed first.
      2) If a key includes 'code' or 'function', 
         the value is treated as a code block.
      3) Otherwise, key-value pairs are displayed as text.
    """
    # 1) Grab the report title (or default)
    title = report_dict.get("report_title", "Untitled Report")

    lines = []
    lines.append(f"# {title}")

    # 2) Iterate over all other keys
    for key, value in report_dict.items():
        # Skip the title key, since we already displayed it
        if key == "report_title":
            continue

        # 3) Check if it's code or function
        # (You can tweak this logic if you have different rules)
        key_lower = key.lower()
        if "code" in key_lower or "function" in key_lower:
            # Treat as code
            lines.append(f"\n## {format_agent_name(key).upper()}")
            lines.append(f"```{code_lang}\n" + str(value) + "\n```")
        else:
            # 4) Otherwise, just display the key-value as text
            lines.append(f"\n## {format_agent_name(key).upper()}")
            lines.append(str(value))

    return "\n".join(lines)
```

This function:

1. Takes a dictionary (typically from parsing JSON) and converts it to a formatted string.
2. Uses the `report_title` field (if present) as the main title.
3. For each key-value pair:
   - If the key contains "code" or "function", formats the value as a code block.
   - Otherwise, formats the value as plain text.
4. Returns the formatted string.

This function is used to create formatted summaries of the agent's workflow and outputs.

### logging.py

The `logging.py` module provides utility functions for logging generated code to files. The data cleaning agent uses the `log_ai_function` function from this module:

```python
def log_ai_function(response: str, file_name: str, log: bool = True, log_path: str = './logs/', overwrite: bool = True):
    """
    Logs the response of an AI function to a file.
    """
    
    if log:
        # Ensure the directory exists
        os.makedirs(log_path, exist_ok=True)

        # file_name = 'data_wrangler.py'
        file_path = os.path.join(log_path, file_name)

        if not overwrite:
            # If file already exists and we're NOT overwriting, we create a new name
            if os.path.exists(file_path):
                # Use an incremental suffix (e.g., data_wrangler_1.py, data_wrangler_2.py, etc.)
                # or a time-based suffix if you prefer.
                base_name, ext = os.path.splitext(file_name)
                i = 1
                while True:
                    new_file_name = f"{base_name}_{i}{ext}"
                    new_file_path = os.path.join(log_path, new_file_name)
                    if not os.path.exists(new_file_path):
                        file_path = new_file_path
                        file_name = new_file_name
                        break
                    i += 1

        # Write the file
        with open(file_path, 'w', encoding='utf-8') as file:
            file.write(response)

        print(f"      File saved to: {file_path}")
        
        return (file_path, file_name)
    
    else:
        return (None, None)
```

This function:

1. Creates the log directory if it doesn't exist.
2. Constructs the file path from the log path and file name.
3. If `overwrite` is `False` and the file already exists, creates a new file name with an incremental suffix.
4. Writes the response to the file.
5. Returns the file path and name.

This function is used to log the generated data cleaning function to a file, making it available for future use.

## Tools

The data cleaning agent uses the `dataframe.py` module from the tools package, which provides functions for working with pandas DataFrames:

### get_dataframe_summary

```python
def get_dataframe_summary(
    dataframes: Union[pd.DataFrame, List[pd.DataFrame], Dict[str, pd.DataFrame]],
    n_sample: int = 30,
    skip_stats: bool = False,
) -> List[str]:
    """
    Generate a summary for one or more DataFrames.
    """
    summaries = []

    # --- Dictionary Case ---
    if isinstance(dataframes, dict):
        for dataset_name, df in dataframes.items():
            summaries.append(_summarize_dataframe(df, dataset_name, n_sample, skip_stats))

    # --- Single DataFrame Case ---
    elif isinstance(dataframes, pd.DataFrame):
        summaries.append(_summarize_dataframe(dataframes, "Single_Dataset", n_sample, skip_stats))

    # --- List of DataFrames Case ---
    elif isinstance(dataframes, list):
        for idx, df in enumerate(dataframes):
            dataset_name = f"Dataset_{idx}"
            summaries.append(_summarize_dataframe(df, dataset_name, n_sample, skip_stats))

    else:
        raise TypeError(
            "Input must be a single DataFrame, a list of DataFrames, or a dictionary of DataFrames."
        )

    return summaries


def _summarize_dataframe(
    df: pd.DataFrame, 
    dataset_name: str, 
    n_sample=30, 
    skip_stats=False
) -> str:
    """Generate a summary string for a single DataFrame."""
    # 1. Convert dictionary-type cells to strings
    #    This prevents unhashable dict errors during df.nunique().
    df = df.apply(lambda col: col.map(lambda x: str(x) if isinstance(x, dict) else x))
    
    # 2. Capture df.info() output
    buffer = io.StringIO()
    df.info(buf=buffer)
    info_text = buffer.getvalue()

    # 3. Calculate missing value stats
    missing_stats = (df.isna().sum() / len(df) * 100).sort_values(ascending=False)
    missing_summary = "\n".join([f"{col}: {val:.2f}%" for col, val in missing_stats.items()])

    # 4. Get column data types
    column_types = "\n".join([f"{col}: {dtype}" for col, dtype in df.dtypes.items()])

    # 5. Get unique value counts
    unique_counts = df.nunique()  # Will no longer fail on unhashable dict
    unique_counts_summary = "\n".join([f"{col}: {count}" for col, count in unique_counts.items()])

    # 6. Generate the summary text
    if not skip_stats:
        summary_text = f"""
        Dataset Name: {dataset_name}
        ----------------------------
        Shape: {df.shape[0]} rows x {df.shape[1]} columns

        Column Data Types:
        {column_types}

        Missing Value Percentage:
        {missing_summary}

        Unique Value Counts:
        {unique_counts_summary}

        Data (first {n_sample} rows):
        {df.head(n_sample).to_string()}

        Data Description:
        {df.describe().to_string()}

        Data Info:
        {info_text}
        """
    else:
        summary_text = f"""
        Dataset Name: {dataset_name}
        ----------------------------
        Shape: {df.shape[0]} rows x {df.shape[1]} columns

        Column Data Types:
        {column_types}

        Data (first {n_sample} rows):
        {df.head(n_sample).to_string()}
        """
        
    return summary_text.strip()
```

These functions:

1. Generate a summary for one or more DataFrames, handling single DataFrames, lists of DataFrames, and dictionaries of DataFrames.
2. For each DataFrame, the summary includes:
   - The shape of the DataFrame (rows and columns)
   - Column data types
   - Missing value percentages
   - Unique value counts
   - The first `n_sample` rows
   - Descriptive statistics (unless `skip_stats` is `True`)
   - The result of `df.info()` (unless `skip_stats` is `True`)

This function is used to generate a summary of the input data for the language model, enabling it to understand the characteristics of the data and recommend appropriate cleaning steps.

## How These Supporting Modules Are Used in the Data Cleaning Agent

The data cleaning agent uses these supporting modules as follows:

1. **Parsers**: The `PythonOutputParser` is used to extract clean Python code from the language model's response when generating the data cleaning function and when fixing broken code.

2. **Utils**:
   - `relocate_imports_inside_function` and `add_comments_to_top` are used to process the generated data cleaning function.
   - `format_agent_name` is used to format the agent's name in log messages and output reports.
   - `format_recommended_steps` is used to format the recommended cleaning steps.
   - `get_generic_summary` is used to create formatted summaries of the agent's workflow and outputs.
   - `log_ai_function` is used to log the generated data cleaning function to a file.

3. **Tools**: The `get_dataframe_summary` function is used to generate a summary of the input data for the language model, enabling it to understand the characteristics of the data and recommend appropriate cleaning steps.

These supporting modules provide essential functionality for the data cleaning agent, enabling it to process language model outputs, log generated code, and understand the characteristics of the input data.
