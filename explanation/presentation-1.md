# AI Data Science Agents: Project Presentation

## Introduction (1 minute)

Good afternoon. Today I'll be demonstrating our AI Data Science agents system, focusing on the data cleaning and data loading capabilities we've implemented.

This presentation will cover:
1. Live demonstration of the agents in action
2. How to use them in your own workflows
3. The architecture and implementation details
4. Testing and reliability measures
5. Next steps in our development roadmap

## Demonstration: Data Cleaning Agent (4 minutes)

Let's start by seeing the data cleaning agent in action. We've developed a robust agent that can automatically analyze, clean, and transform datasets with minimal human input.

Here's a snippet from our demonstration example:

```python
# Import necessary components
from langchain_openai import ChatOpenAI
from src.agents.data_cleaning_agent import DataCleaningAgent
import pandas as pd
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

# Load the dataset
df = pd.read_csv("data/samples/train.csv")
print(f"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns")

# Create the data cleaning agent
llm = ChatOpenAI(
    model="gpt-4o-mini", 
    openai_api_key=openai_api_key,
    temperature=0.2  # Lower temperature for more deterministic results
)

cleaning_agent = DataCleaningAgent(
    model=llm,
    n_samples=50,  # We'll examine 50 rows of data
    log=True,      # Log the process for debugging
    log_path="logs",
    file_name="housing_data_cleaner.py",
    function_name="clean_housing_data"
)

# Invoke the agent with specific instructions
cleaning_instructions = """
Clean this housing dataset by:
1. Handling missing values appropriately:
   - For numeric columns with < 20% missing, impute with median
   - For categorical columns, impute with mode
   - For columns with > 40% missing, consider dropping
2. Handle outliers in numeric columns
3. Convert categorical variables to appropriate types
4. Ensure all data types are appropriate
5. Add helpful comments explaining your cleaning decisions
"""

# Run the cleaning process
result = cleaning_agent.invoke_agent(
    data_raw=df,
    user_instructions=cleaning_instructions,
    max_retries=3
)

# Examine the cleaned data
cleaned_df = cleaning_agent.get_data_cleaned()
print(f"Cleaned data shape: {cleaned_df.shape}")

# Compare missing values before and after
comparison = pd.DataFrame({
    'Original Missing': df.isnull().sum(),
    'Cleaned Missing': cleaned_df.isnull().sum(),
    'Original %': (df.isnull().sum() / len(df) * 100).round(2),
    'Cleaned %': (cleaned_df.isnull().sum() / len(cleaned_df) * 100).round(2)
})
print(comparison[comparison['Original Missing'] > 0])

# Display the generated cleaning code
cleaning_code = cleaning_agent.get_data_cleaner_function()
print("\nGenerated data cleaning code:")
print(cleaning_code)
```

As you can see, the agent has:
1. Analyzed the dataset to identify issues like missing values and outliers
2. Generated appropriate Python code based on our instructions
3. Applied the cleaning operations successfully
4. Provided us with both the cleaned data and the reusable cleaning function

The most powerful aspect here is that we only needed to provide high-level instructions - the agent determined the appropriate techniques based on analyzing the data.

## Benefits of the Data Cleaning Agent (2 minutes)

Our data cleaning agent offers several key benefits:

1. **Automatic analysis**: The agent examines your data to identify issues without requiring manual inspection.

2. **Customizable cleaning**: You can provide specific instructions or let the agent use best practices.

3. **Transparent process**: The agent generates human-readable Python code, so you can see exactly what transformations were applied.

4. **Error handling**: Built-in error detection and correction mechanisms ensure robust processing.

5. **Reproducibility**: The generated cleaning function can be saved and reused on similar datasets.

Here's a real example of a data cleaning function generated by our agent:

```python
def clean_housing_data(data_raw):
    """
    Clean the housing dataset by handling missing values, removing outliers,
    converting data types, and preparing for analysis.
    
    Args:
        data_raw (pd.DataFrame): Raw housing data
        
    Returns:
        pd.DataFrame: Cleaned housing data
    """
    import pandas as pd
    import numpy as np
    from sklearn.impute import SimpleImputer
    
    # Create a copy of the dataframe to avoid modifying the original
    df = data_raw.copy()
    
    # Step 1: Handle missing values
    
    # For LotFrontage (19.5% missing), impute with median grouped by neighborhood
    lot_frontage_medians = df.groupby('Neighborhood')['LotFrontage'].median()
    for neighborhood in df['Neighborhood'].unique():
        mask = (df['Neighborhood'] == neighborhood) & (df['LotFrontage'].isna())
        df.loc[mask, 'LotFrontage'] = lot_frontage_medians[neighborhood]
    # Fill remaining missing values with overall median
    df['LotFrontage'].fillna(df['LotFrontage'].median(), inplace=True)
    
    # For MasVnrArea (missing in 0.5% of rows), impute with 0 as it likely means no masonry veneer
    df['MasVnrArea'].fillna(0, inplace=True)
    
    # For Electrical (1 missing value), impute with mode
    df['Electrical'].fillna(df['Electrical'].mode()[0], inplace=True)
    
    # For categorical columns with significant missing values, fill with 'None'
    # These are typically features where absence indicates the feature doesn't exist
    categorical_cols_to_fill = ['MasVnrType', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 
                               'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu',
                               'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',
                               'PoolQC', 'Fence', 'MiscFeature', 'Alley']
    for col in categorical_cols_to_fill:
        if col in df.columns:
            df[col].fillna('None', inplace=True)
    
    # For GarageYrBlt, fill missing with YearBuilt as an approximation
    df['GarageYrBlt'].fillna(df['YearBuilt'], inplace=True)
    
    # Step 2: Remove outliers (using IQR method for numeric columns)
    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
    
    # Skip ID columns and sale price (target variable)
    cols_to_check = [col for col in numeric_cols if col not in ['Id', 'SalePrice']]
    
    for col in cols_to_check:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 3 * IQR
        upper_bound = Q3 + 3 * IQR
        
        # Flag outliers but don't remove them to preserve as much data as possible
        # Just cap extreme values
        df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])
        df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])
    
    # Step 3: Convert data types
    
    # Convert categorical variables to appropriate types
    categorical_cols = df.select_dtypes(include=['object']).columns
    for col in categorical_cols:
        df[col] = df[col].astype('category')
    
    # Ensure year columns are integers
    year_cols = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold']
    for col in year_cols:
        if col in df.columns:
            df[col] = df[col].astype(int)
    
    return df
```

## Demonstration: Data Loading Agent (4 minutes)

Another powerful component of our system is the data loading agent, which can automatically discover, load, and prepare data from various sources.

```python
from src.agents.data_loader_tools_agent import DataLoaderToolsAgent

# Create the data loader agent
data_loader_agent = DataLoaderToolsAgent(
    model=llm,
    log=True,
    log_path="logs"
)

# Ask it to load and analyze files from a directory
result = data_loader_agent.invoke_agent(
    user_instructions="Load all CSV files from the data/samples directory, analyze their schema, and prepare them for merging.",
    directory_path="data/samples",
    max_retries=2
)

# The agent identifies files, loads them, and analyzes their structure
loaded_data = data_loader_agent.get_data_loaded()
print("\nLoaded Data Files:")
for file_name, data in loaded_data.items():
    print(f"- {file_name}: {len(data)} rows, {list(data.columns)} columns")

# The agent can also provide schema information
schema_info = data_loader_agent.get_schema_info()
print("\nSchema Analysis:")
print(schema_info)

# And generate code to transform the data if needed
transform_code = data_loader_agent.get_transform_code()
print("\nTransformation Code:")
print(transform_code)
```

The data loader agent can:
1. Discover and load files in various formats (CSV, JSON, Excel, etc.)
2. Automatically detect schemas and data types
3. Generate code to transform and prepare data
4. Handle multiple files simultaneously

## Using Agents with µAgents Framework (2 minutes)

Both agents can be integrated with the µAgents framework for distributed operation:

```python
from src.adapters.uagents_adapter import register_agent_as_uagent

# Register the data cleaning agent as a µAgent
cleaning_uagent = register_agent_as_uagent(
    agent=cleaning_agent,
    endpoint="http://localhost:8000/data-cleaning",
    agent_name="data_cleaning_agent",
    agent_description="AI agent for data cleaning operations"
)

# Register the data loader agent as a µAgent
loader_uagent = register_agent_as_uagent(
    agent=data_loader_agent,
    endpoint="http://localhost:8001/data-loading",
    agent_name="data_loader_agent",
    agent_description="AI agent for data loading and transformation"
)

# Start the agents
cleaning_uagent.run()
loader_uagent.run()
```

This enables:
- REST API endpoints for each agent
- Asynchronous processing
- Integration with other systems
- Distributed operation across machines if needed

## Creating a Data Pipeline (2 minutes)

We can also chain these agents together to create a complete data pipeline:

```python
from src.examples.data_pipeline_example import create_data_pipeline

# Define the pipeline stages
pipeline = create_data_pipeline(
    llm=llm,
    input_directory="data/raw",
    output_directory="data/processed",
    cleaning_instructions="Handle missing values, remove duplicates, and convert dates to ISO format",
    loader_instructions="Load all CSV and JSON files, standardize schemas"
)

# Execute the pipeline
pipeline_result = pipeline.invoke({
    "input_path": "data/raw/monthly_reports",
    "output_format": "parquet"
})

print("Pipeline execution complete!")
print(f"Processed {pipeline_result['files_processed']} files")
print(f"Output saved to: {pipeline_result['output_path']}")
```

This demonstrates how easily these agents can be composed into more complex workflows.

## Architecture Overview (5 minutes)

Let's look at how the system is architected:

### Core Components:

1. **Base Agent Framework:**
```python
# src/templates/agent_templates.py
class BaseAgent:
    """Base class for all agents in the system."""
    
    def __init__(self, model, log=False, log_path=None, **kwargs):
        self._params = {
            "model": model,
            "log": log,
            "log_path": log_path,
            **kwargs
        }
        self._compiled_graph = None
        # Initialize other components
```

2. **LangGraph Workflow:**
```python
# src/templates/agent_templates.py
def create_coding_agent_graph(node_functions, entry_point="recommend_steps"):
    """Creates a LangGraph workflow for coding agents."""
    
    workflow = StateGraph(AgentState)
    
    # Add nodes to the graph
    for name, func in node_functions.items():
        workflow.add_node(name, func)
    
    # Add edges to create the flow
    workflow.add_edge(entry_point, "create_code")
    workflow.add_edge("create_code", "execute_code")
    workflow.add_conditional_edges(
        "execute_code",
        lambda x: "error" in x["execution_result"],
        {
            True: "fix_code",
            False: "report_results"
        }
    )
    workflow.add_edge("fix_code", "execute_code")
    workflow.add_edge("report_results", END)
    
    # Set the entry point
    workflow.set_entry_point(entry_point)
    
    return workflow.compile()
```

3. **Tools Implementation:**
```python
# src/tools/dataframe.py
def get_dataframe_summary(df, n_samples=5):
    """Generate a comprehensive summary of a pandas DataFrame."""
    
    if not isinstance(df, pd.DataFrame):
        return {"error": "Input is not a pandas DataFrame"}
    
    summary = {
        "shape": df.shape,
        "columns": list(df.columns),
        "dtypes": {col: str(dtype) for col, dtype in df.dtypes.items()},
        "missing_values": df.isna().sum().to_dict(),
        "samples": df.head(n_samples).to_dict(),
        "description": df.describe().to_dict()
    }
    
    return summary
```

4. **Agent Implementation:**
```python
# src/agents/data_cleaning_agent.py
def make_data_cleaning_agent(model, n_samples=10, log=False, log_path=None):
    """Create a data cleaning agent with LangGraph."""
    
    # Create prompts
    recommend_prompt = ChatPromptTemplate.from_messages([
        ("system", SYSTEM_RECOMMEND_STEPS_TEMPLATE),
        ("user", RECOMMEND_STEPS_TEMPLATE)
    ])
    
    # Define node functions
    def recommend_cleaning_steps(state):
        # Analyze data and recommend cleaning steps
        # ...
        
    def create_data_cleaner_code(state):
        # Generate Python code for data cleaning
        # ...
    
    # Create and return the graph
    # ...
```

### Key Architecture Patterns:

1. **Agent Composition:**
   - All agents inherit from BaseAgent
   - Standardized interfaces for invocation
   - Common patterns for error handling

2. **LLM Delegation Pattern:**
   - Complex tasks broken into specialized sub-tasks
   - Each node responsible for one aspect of processing
   - Clear flow of information between steps

3. **Tool Integration:**
   - Tools are self-contained and reusable
   - Agents leverage tools rather than reimplementing functionality
   - New capabilities can be added by developing new tools

4. **µAgents Integration:**
   - Adapter pattern for compatibility with µAgents framework
   - Each agent can be exposed as a REST API endpoint
   - Communication protocol handled by adapter

## Testing and Reliability (2 minutes)

We've implemented comprehensive test suites for all components:

```python
# test_data_cleaning_agent.py
def test_data_cleaning_agent_creation():
    """Test DataCleaningAgent initialization."""
    agent = DataCleaningAgent(model=MockLLM())
    assert agent is not None
    assert agent._params["model"] is not None

def test_data_cleaning_execution():
    """Test data cleaning execution with sample data."""
    df = pd.DataFrame({
        'A': [1, 2, None, 4, 1000],  # Contains outlier and missing value
        'B': ['x', 'y', 'z', None, 'x']  # Contains missing value
    })
    
    agent = DataCleaningAgent(model=MockLLM())
    result = agent.invoke_agent(
        data_raw=df,
        user_instructions="Handle missing values and remove outliers",
        max_retries=2
    )
    
    cleaned_df = agent.get_data_cleaned()
    assert cleaned_df is not None
    assert cleaned_df.isna().sum().sum() == 0  # No missing values
    assert 1000 not in cleaned_df['A'].values  # Outlier removed
```

Our test coverage ensures:
- Agents initialize correctly
- Handling of various data formats
- Error recovery and fault tolerance
- Edge cases (missing data, large datasets, etc.)
- Integration between components

## Summary and Benefits (1 minute)

The AI Data Science agents we've developed provide:

1. **Automation of routine data tasks:**
   - Data cleaning, transformation, and loading automated
   - Consistent, reproducible results

2. **Accessibility:**
   - Non-technical users can leverage advanced data processing
   - Natural language instructions instead of complex code

3. **Flexibility:**
   - Works with various data formats and sources
   - Easily integrated with existing systems

4. **Extensibility:**
   - New agents can be added to the framework
   - Custom tools can extend functionality

Thank you for your attention. I'd be happy to answer any questions or provide more details on specific aspects of the implementation. 