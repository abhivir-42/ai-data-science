# Data Pipeline Report

**File processed:** data/samples/house-price-prediction-train.csv

## Original Data
- Shape: (200, 81)
- Missing values: 1087

## Cleaned Data
- Shape: (200, 81)
- Missing values: 8600

## Cleaning Steps
# Recommended Data Cleaning Steps:

Given the characteristics of the dataset and the user instructions, here are the recommended steps to clean and preprocess the data for house price prediction:

1. **Remove Columns with Excessive Missing Values**:
   * Evaluate each column for missing data. If a column has more than 40% missing values, remove it. 
   ```python
   df.drop(columns=[col for col in df.columns if df[col].isnull().mean() > 0.4], inplace=True)
   ```

2. **Impute Missing Values for Numeric Columns**:
   * For numeric columns with missing values, replace them with the mean of that column.
   ```python
   for col in df.select_dtypes(include=['float64', 'int64']).columns:
       if df[col].isnull().sum() > 0:
           df[col].fillna(df[col].mean(), inplace=True)
   ```

3. **Impute Missing Values for Categorical Columns**:
   * For categorical columns with missing values, fill them with the mode of that column.
   ```python
   for col in df.select_dtypes(include=['object']).columns:
       if df[col].isnull().sum() > 0:
           df[col].fillna(df[col].mode()[0], inplace=True)
   ```

4. **Convert Columns to the Correct Data Types**:
   * Ensure each column is of the appropriate data type given its context (e.g., dates for time-related data). Update any incorrect data types.
   ```python
   df['GarageYrBlt'] = df['GarageYrBlt'].astype('Int64')  # Example for converting to nullable integer.
   ```

5. **Remove Duplicate Rows**:
   * Check for and remove any duplicate rows in the dataset.
   ```python
   df.drop_duplicates(inplace=True)
   ```

6. **Remove Rows with Missing Values (if not imputed)**:
   * This step may not be needed, as we are filling missing values. However, if any columns are excluded from filling, ensure to drop those rows.
   ```python
   df.dropna(inplace=True)  # This is optional depending on previous steps.
   ```

7. **Handle Extreme Outliers**:
   * Identify extreme outliers using IQR and remove any rows that contain these outliers.
   ```python
   Q1 = df.quantile(0.25)
   Q3 = df.quantile(0.75)
   IQR = Q3 - Q1
   df = df[~((df < (Q1 - 3 * IQR)) | (df > (Q3 + 3 * IQR))).any(axis=1)]
   ```

8. **Analyze Data for Additional Cleaning Needs**:
   * Review the dataset for any other peculiarities or trends that may require additional cleaning actions not previously addressed. Examine specific columns unique to the dataset and consider additional preprocessing.

No additional cleaning steps are specifically required beyond those mentioned given the user instructions for basic cleaning. 

These steps should help achieve a clean and manageable dataset suitable for house price prediction modeling.

## Cleaning Function
```python
# ----------------------------------------------------------------------
# Code generated by AI agent: data_cleaning_agent
# ----------------------------------------------------------------------

def data_cleaner(data_raw):
    import pandas as pd
    import numpy as np
    from scipy import stats
    import warnings
    
    # Suppress warnings for clean output
    warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)
    pd.set_option('mode.chained_assignment', None)
    
    # ALWAYS work on a copy to preserve original
    data = data_raw.copy()
    
    # Initialize tracking variables
    original_shape = data.shape
    original_rows = len(data)
    cleaning_log = []
    
    print(f"üßπ Starting data cleaning: {original_shape[0]} rows √ó {original_shape[1]} columns")
    
    try:
        # STEP 1: Data Type Optimization
        print("üìä Step 1: Optimizing data types...")
        for col in data.columns:
            # Convert string representation of numbers to numeric
            if data[col].dtype == 'object':
                try:
                    data[col] = pd.to_numeric(data[col], errors='coerce')  # Convert strings to numbers if possible
                except ValueError:
                    continue  # Skip if it cannot be converted

            # Optimize categorical columns
            if data[col].dtype == 'object':
                data[col] = data[col].astype('category')

        # Re-evaluate the data types after initial conversion
        print("   Data types optimized")
        
        # STEP 2: Handle Missing Values (PRIORITIZE IMPUTATION)
        print("üîß Step 2: Handling missing values...")
        missing_before = data.isnull().sum().sum()
        print(f"   Missing values before cleaning: {missing_before}")
        
        for col in data.columns:
            if data[col].isnull().sum() > 0:
                col_dtype = str(data[col].dtype)
                missing_count = data[col].isnull().sum()
                print(f"   Handling {missing_count} missing values in '{col}' ({col_dtype})")
                
                # Handle numeric columns
                if pd.api.types.is_numeric_dtype(data[col]):
                    if data[col].skew() > 2 or data[col].skew() < -2:  # Consider skewness
                        fill_value = data[col].median()  # Use median for skewed data
                        data[col] = data[col].fillna(fill_value)
                        cleaning_log.append(f"Filled {missing_count} missing values in '{col}' with median ({fill_value})")
                    else:
                        fill_value = data[col].mean()  # Use mean for normal distributions
                        data[col] = data[col].fillna(fill_value)
                        cleaning_log.append(f"Filled {missing_count} missing values in '{col}' with mean ({fill_value:.2f})")
                
                # Handle categorical columns (CRITICAL: Safe categorical handling)
                elif col_dtype == 'category':
                    mode_value = data[col].mode()[0] if not data[col].mode().empty else 'Unknown'
                    data[col] = data[col].fillna(mode_value)
                    cleaning_log.append(f"Filled {missing_count} missing values in '{col}' with mode ({mode_value})")
                
                # Handle object/string columns
                elif col_dtype == 'object':
                    mode_value = data[col].mode()[0] if not data[col].mode().empty else 'Unknown'
                    data[col] = data[col].fillna(mode_value)
                    cleaning_log.append(f"Filled {missing_count} missing values in '{col}' with mode ({mode_value})")
                
                # Handle datetime columns
                elif pd.api.types.is_datetime64_any_dtype(data[col]):
                    data[col] = data[col].fillna(method='ffill')
                    if data[col].isnull().sum() > 0:
                        data[col] = data[col].fillna(method='bfill')
                    cleaning_log.append(f"Filled {missing_count} missing values in '{col}' using forward/backward fill")
        
        missing_after = data.isnull().sum().sum()
        print(f"   Missing values after cleaning: {missing_after}")
        if missing_after < missing_before:
            print(f"   Successfully reduced missing values by {missing_before - missing_after}")
        elif missing_after > 0:
            print(f"   ‚ö†Ô∏è Warning: {missing_after} missing values remain")
        
        # STEP 3: Remove Duplicates
        print("üóëÔ∏è Step 3: Removing duplicates...")
        duplicates_before = data.duplicated().sum()
        if duplicates_before > 0:
            data = data.drop_duplicates()
            cleaning_log.append(f"Removed {duplicates_before} duplicate rows")
        
        # STEP 4: Handle Outliers (**Conservative approach, if deemed necessary**)
        print("üìà Step 4: Conservative outlier handling...")
        # Outlier handling logic could be integrated here with caution if requested
        
        # STEP 5: Final Validation and Cleanup
        print("‚úÖ Step 5: Final validation...")
        # Remove any remaining rows that are completely empty
        empty_rows = data.isnull().all(axis=1).sum()
        if empty_rows > 0:
            data = data.dropna(how='all')
            cleaning_log.append(f"Removed {empty_rows} completely empty rows")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Error during cleaning: {str(e)}")
        print("üîÑ Returning original data to prevent data loss")
        return data_raw.copy()
    
    # MANDATORY: Data preservation validation
    final_shape = data.shape
    final_rows = len(data)
    data_loss_pct = ((original_rows - final_rows) / original_rows) * 100 if original_rows > 0 else 0
    
    # Comprehensive reporting
    print(f"\nüìà CLEANING SUMMARY:")
    print(f"   Original: {original_shape[0]} rows √ó {original_shape[1]} columns")
    print(f"   Final: {final_shape[0]} rows √ó {final_shape[1]} columns")
    print(f"   Data retention: {100-data_loss_pct:.1f}%")
    
    if cleaning_log:
        print(f"\nüìù Actions taken:")
        for action in cleaning_log:
            print(f"   ‚Ä¢ {action}")
    
    # Critical validation checks
    if data_loss_pct > 20:
        print(f"üö® CRITICAL WARNING: High data loss ({data_loss_pct:.1f}%)!")
        print(f"   Consider using more conservative cleaning approaches")
    
    if final_rows == 0:
        print(f"‚ùå FATAL ERROR: All data was removed! Returning original data")
        return data_raw.copy()
    
    if final_rows < 10 and original_rows > 100:
        print(f"‚ö†Ô∏è WARNING: Extreme data reduction ({final_rows} remaining from {original_rows})")
    
    # Reset pandas options
    pd.set_option('mode.chained_assignment', 'warn')
    warnings.resetwarnings()
    
    print(f"‚úÖ Data cleaning completed successfully!")
    return data
```
