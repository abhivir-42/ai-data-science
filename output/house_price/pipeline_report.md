# Data Pipeline Report

**File processed:** data/samples/house-price-prediction-train.csv

## Original Data
- Shape: (200, 81)
- Missing values: 1087

## Cleaned Data
- Shape: (200, 75)
- Missing values: 51

## Cleaning Steps
# Recommended Data Cleaning Steps:

Here are the recommended steps to clean and preprocess the provided house price prediction dataset:

### Data Cleaning Steps:

1. **Remove Columns with Excessive Missing Values:**
   - Check for columns with more than 40% missing values and remove those columns. 
   - **Reason:** These columns have insufficient data to contribute meaningfully to the analysis.

   ```python
   # Example: df.drop(columns=[col_name], inplace=True)
   ```

2. **Impute Missing Numeric Values:**
   - For numeric columns (e.g., `LotFrontage`, `MasVnrArea`), impute missing values with the mean of the column.
   - **Reason:** This helps retain the data while minimizing distortion of the column's distribution.

   ```python
   # Example: df[numeric_col].fillna(df[numeric_col].mean(), inplace=True)
   ```

3. **Impute Missing Categorical Values:**
   - For categorical columns (e.g., `MasVnrType`, `FireplaceQu`, `GarageType`), impute missing values with the mode of the column.
   - **Reason:** This maintains the categorical nature of the data and addresses missing entries in a reasonable way.

   ```python
   # Example: df[categorical_col].fillna(df[categorical_col].mode()[0], inplace=True)
   ```

4. **Convert Columns to Correct Data Types:**
   - Convert `GarageYrBlt` to integer or categorical as appropriate.
   - **Reason:** Ensures that each column is in the correct format for analysis.

   ```python
   # Example: df['GarageYrBlt'] = df['GarageYrBlt'].astype(int)
   ```

5. **Remove Duplicate Rows:**
   - Identify and remove duplicate rows in the DataFrame.
   - **Reason:** Duplicates can skew the analysis and accuracy of predictions.

   ```python
   # Example: df.drop_duplicates(inplace=True)
   ```

6. **Remove Rows with Missing Values (if any remain):**
   - Check if there are any rows with missing values after imputing and consider removing them.
   - **Reason:** Ensures that all included data points are complete.

   ```python
   # Example: df.dropna(inplace=True)
   ```

7. **Remove Extreme Outliers:**
   - Identify and remove any rows with extreme outliers based on a rule of 3 times the interquartile range (IQR).
   - **Reason:** Outliers can significantly affect model performance and skew results.

   ```python
   # Example: Use IQR to filter out outliers
   ```

8. **Analyze for Additional Cleaning Needs:**
   - Perform exploratory data analysis to identify if there are any additional cleaning needs, such as highly imbalanced categorical data or other transformations.
   - **Reason:** Tailors the cleaning process to the specific characteristics of the dataset.

### Summary of User-Specific Instructions:
- The steps respect the user's request for basic cleaning for house price prediction, focusing on filling missing values and handling categorical data simply.
- Any steps not included (such as saving files or unrequested operations) align with the user instructions to avoid unrelated content.

This structured cleaning process ensures that the dataset is adequately prepared for predictive modeling, fostering improved accuracy and efficiency in deriving insights from house prices.

## Cleaning Function
```python
# ----------------------------------------------------------------------
# Code generated by AI agent: data_cleaning_agent
# ----------------------------------------------------------------------

def data_cleaner(data_raw):
    import pandas as pd
    import numpy as np
    from scipy import stats
    import warnings
    
    # Suppress pandas warnings for clean output
    warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)
    pd.set_option('mode.chained_assignment', None)
    
    # ALWAYS work on a copy to preserve original
    data = data_raw.copy()
    
    # Initialize tracking variables
    original_shape = data.shape
    original_rows = len(data)
    cleaning_log = []
    
    print(f"üßπ Starting data cleaning: {original_shape[0]} rows √ó {original_shape[1]} columns")

    try:
        # STEP 1: Data Type Optimization
        print("üìä Step 1: Optimizing data types...")
        
        # Convert object types to categories where applicable
        for col in data.select_dtypes(include='object').columns:
            if data[col].nunique() < 0.5 * len(data):
                data[col] = data[col].astype('category')

        # Optimize numeric columns to smaller dtypes
        for col in data.select_dtypes(include=['int64', 'float64']).columns:
            if pd.api.types.is_integer_dtype(data[col]):
                data[col] = pd.to_numeric(data[col], downcast='integer')
            else:
                data[col] = pd.to_numeric(data[col], downcast='float')

        # STEP 2: Handle Missing Values (PRIORITIZE IMPUTATION)
        print("üîß Step 2: Handling missing values...")
        
        # Step 2a: Remove columns with more than 40% missing values
        missing_percentage = data.isnull().mean() * 100
        columns_to_remove = missing_percentage[missing_percentage > 40].index
        if len(columns_to_remove) > 0:
            data.drop(columns=columns_to_remove, inplace=True)
            cleaning_log.append(f"Removed columns: {list(columns_to_remove)} with > 40% missing values")
        
        # Step 2b: Impute missing numeric values with the mean
        for numeric_col in data.select_dtypes(include=['float64', 'int64']).columns:
            if data[numeric_col].isnull().any():
                mean_value = data[numeric_col].mean()
                data[numeric_col].fillna(mean_value, inplace=True)
                cleaning_log.append(f"Imputed missing values in {numeric_col} with mean {mean_value:.2f}")

        # Step 2c: Impute missing categorical values with the mode
        for categorical_col in data.select_dtypes(include='category').columns:
            if data[categorical_col].isnull().any():
                mode_value = data[categorical_col].mode()[0]
                data[categorical_col].fillna(mode_value, inplace=True)
                cleaning_log.append(f"Imputed missing values in {categorical_col} with mode '{mode_value}'")
        
        # STEP 3: Remove Duplicates
        print("üóëÔ∏è Step 3: Removing duplicates...")
        duplicates_before = data.duplicated().sum()
        if duplicates_before > 0:
            data = data.drop_duplicates()
            cleaning_log.append(f"Removed {duplicates_before} duplicate rows")
        
        # STEP 4: Handle Outliers (CONSERVATIVE)
        print("üìà Step 4: Conservative outlier handling...")
        # Example implementation, more could be added based on domain knowledge as needed.
        # Only commented as outlier handling is conservative and not always done.
        
        # STEP 5: Final Validation and Cleanup
        print("‚úÖ Step 5: Final validation...")
        # Remove any remaining rows that are completely empty
        empty_rows = data.isnull().all(axis=1).sum()
        if empty_rows > 0:
            data = data.dropna(how='all')
            cleaning_log.append(f"Removed {empty_rows} completely empty rows")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Error during cleaning: {str(e)}")
        print("üîÑ Returning original data to prevent data loss")
        return data_raw.copy()
    
    # MANDATORY: Data preservation validation
    final_shape = data.shape
    final_rows = len(data)
    data_loss_pct = ((original_rows - final_rows) / original_rows) * 100 if original_rows > 0 else 0
    
    # Comprehensive reporting
    print(f"\nüìà CLEANING SUMMARY:")
    print(f"   Original: {original_shape[0]} rows √ó {original_shape[1]} columns")
    print(f"   Final: {final_shape[0]} rows √ó {final_shape[1]} columns")
    print(f"   Data retention: {100-data_loss_pct:.1f}%")
    
    if cleaning_log:
        print(f"\nüìù Actions taken:")
        for action in cleaning_log:
            print(f"   ‚Ä¢ {action}")
    
    # Critical validation checks
    if data_loss_pct > 25:
        print(f"üö® CRITICAL WARNING: High data loss ({data_loss_pct:.1f}%)!")
    
    if final_rows == 0:
        print(f"‚ùå FATAL ERROR: All data was removed! Returning original data")
        return data_raw.copy()
    
    if final_rows < 10 and original_rows > 100:
        print(f"‚ö†Ô∏è WARNING: Extreme data reduction ({final_rows} remaining from {original_rows})")
    
    # Reset pandas options
    pd.set_option('mode.chained_assignment', 'warn')
    warnings.resetwarnings()
    
    print(f"‚úÖ Data cleaning completed successfully!")
    return data
```
