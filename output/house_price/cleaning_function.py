# ----------------------------------------------------------------------
# Code generated by AI agent: data_cleaning_agent
# ----------------------------------------------------------------------

def data_cleaner(data_raw):
    import pandas as pd
    import numpy as np
    from scipy import stats
    import warnings
    
    # Suppress warnings for clean output
    warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)
    pd.set_option('mode.chained_assignment', None)
    
    # ALWAYS work on a copy to preserve original
    data = data_raw.copy()
    
    # Initialize tracking variables
    original_shape = data.shape
    original_rows = len(data)
    cleaning_log = []
    
    print(f"üßπ Starting data cleaning: {original_shape[0]} rows √ó {original_shape[1]} columns")
    
    try:
        # STEP 1: Data Type Optimization
        print("üìä Step 1: Optimizing data types...")
        for col in data.columns:
            # Convert string representation of numbers to numeric
            if data[col].dtype == 'object':
                try:
                    data[col] = pd.to_numeric(data[col], errors='coerce')  # Convert strings to numbers if possible
                except ValueError:
                    continue  # Skip if it cannot be converted

            # Optimize categorical columns
            if data[col].dtype == 'object':
                data[col] = data[col].astype('category')

        # Re-evaluate the data types after initial conversion
        print("   Data types optimized")
        
        # STEP 2: Handle Missing Values (PRIORITIZE IMPUTATION)
        print("üîß Step 2: Handling missing values...")
        missing_before = data.isnull().sum().sum()
        print(f"   Missing values before cleaning: {missing_before}")
        
        for col in data.columns:
            if data[col].isnull().sum() > 0:
                col_dtype = str(data[col].dtype)
                missing_count = data[col].isnull().sum()
                print(f"   Handling {missing_count} missing values in '{col}' ({col_dtype})")
                
                # Handle numeric columns
                if pd.api.types.is_numeric_dtype(data[col]):
                    if data[col].skew() > 2 or data[col].skew() < -2:  # Consider skewness
                        fill_value = data[col].median()  # Use median for skewed data
                        data[col] = data[col].fillna(fill_value)
                        cleaning_log.append(f"Filled {missing_count} missing values in '{col}' with median ({fill_value})")
                    else:
                        fill_value = data[col].mean()  # Use mean for normal distributions
                        data[col] = data[col].fillna(fill_value)
                        cleaning_log.append(f"Filled {missing_count} missing values in '{col}' with mean ({fill_value:.2f})")
                
                # Handle categorical columns (CRITICAL: Safe categorical handling)
                elif col_dtype == 'category':
                    mode_value = data[col].mode()[0] if not data[col].mode().empty else 'Unknown'
                    data[col] = data[col].fillna(mode_value)
                    cleaning_log.append(f"Filled {missing_count} missing values in '{col}' with mode ({mode_value})")
                
                # Handle object/string columns
                elif col_dtype == 'object':
                    mode_value = data[col].mode()[0] if not data[col].mode().empty else 'Unknown'
                    data[col] = data[col].fillna(mode_value)
                    cleaning_log.append(f"Filled {missing_count} missing values in '{col}' with mode ({mode_value})")
                
                # Handle datetime columns
                elif pd.api.types.is_datetime64_any_dtype(data[col]):
                    data[col] = data[col].fillna(method='ffill')
                    if data[col].isnull().sum() > 0:
                        data[col] = data[col].fillna(method='bfill')
                    cleaning_log.append(f"Filled {missing_count} missing values in '{col}' using forward/backward fill")
        
        missing_after = data.isnull().sum().sum()
        print(f"   Missing values after cleaning: {missing_after}")
        if missing_after < missing_before:
            print(f"   Successfully reduced missing values by {missing_before - missing_after}")
        elif missing_after > 0:
            print(f"   ‚ö†Ô∏è Warning: {missing_after} missing values remain")
        
        # STEP 3: Remove Duplicates
        print("üóëÔ∏è Step 3: Removing duplicates...")
        duplicates_before = data.duplicated().sum()
        if duplicates_before > 0:
            data = data.drop_duplicates()
            cleaning_log.append(f"Removed {duplicates_before} duplicate rows")
        
        # STEP 4: Handle Outliers (**Conservative approach, if deemed necessary**)
        print("üìà Step 4: Conservative outlier handling...")
        # Outlier handling logic could be integrated here with caution if requested
        
        # STEP 5: Final Validation and Cleanup
        print("‚úÖ Step 5: Final validation...")
        # Remove any remaining rows that are completely empty
        empty_rows = data.isnull().all(axis=1).sum()
        if empty_rows > 0:
            data = data.dropna(how='all')
            cleaning_log.append(f"Removed {empty_rows} completely empty rows")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Error during cleaning: {str(e)}")
        print("üîÑ Returning original data to prevent data loss")
        return data_raw.copy()
    
    # MANDATORY: Data preservation validation
    final_shape = data.shape
    final_rows = len(data)
    data_loss_pct = ((original_rows - final_rows) / original_rows) * 100 if original_rows > 0 else 0
    
    # Comprehensive reporting
    print(f"\nüìà CLEANING SUMMARY:")
    print(f"   Original: {original_shape[0]} rows √ó {original_shape[1]} columns")
    print(f"   Final: {final_shape[0]} rows √ó {final_shape[1]} columns")
    print(f"   Data retention: {100-data_loss_pct:.1f}%")
    
    if cleaning_log:
        print(f"\nüìù Actions taken:")
        for action in cleaning_log:
            print(f"   ‚Ä¢ {action}")
    
    # Critical validation checks
    if data_loss_pct > 20:
        print(f"üö® CRITICAL WARNING: High data loss ({data_loss_pct:.1f}%)!")
        print(f"   Consider using more conservative cleaning approaches")
    
    if final_rows == 0:
        print(f"‚ùå FATAL ERROR: All data was removed! Returning original data")
        return data_raw.copy()
    
    if final_rows < 10 and original_rows > 100:
        print(f"‚ö†Ô∏è WARNING: Extreme data reduction ({final_rows} remaining from {original_rows})")
    
    # Reset pandas options
    pd.set_option('mode.chained_assignment', 'warn')
    warnings.resetwarnings()
    
    print(f"‚úÖ Data cleaning completed successfully!")
    return data