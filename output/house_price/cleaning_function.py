# ----------------------------------------------------------------------
# Code generated by AI agent: data_cleaning_agent
# ----------------------------------------------------------------------

def data_cleaner(data_raw):
    import pandas as pd
    import numpy as np
    from scipy import stats
    import warnings
    
    # Suppress pandas warnings for clean output
    warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)
    pd.set_option('mode.chained_assignment', None)
    
    # ALWAYS work on a copy to preserve original
    data = data_raw.copy()
    
    # Initialize tracking variables
    original_shape = data.shape
    original_rows = len(data)
    cleaning_log = []
    
    print(f"üßπ Starting data cleaning: {original_shape[0]} rows √ó {original_shape[1]} columns")

    try:
        # STEP 1: Data Type Optimization
        print("üìä Step 1: Optimizing data types...")
        
        # Convert object types to categories where applicable
        for col in data.select_dtypes(include='object').columns:
            if data[col].nunique() < 0.5 * len(data):
                data[col] = data[col].astype('category')

        # Optimize numeric columns to smaller dtypes
        for col in data.select_dtypes(include=['int64', 'float64']).columns:
            if pd.api.types.is_integer_dtype(data[col]):
                data[col] = pd.to_numeric(data[col], downcast='integer')
            else:
                data[col] = pd.to_numeric(data[col], downcast='float')

        # STEP 2: Handle Missing Values (PRIORITIZE IMPUTATION)
        print("üîß Step 2: Handling missing values...")
        
        # Step 2a: Remove columns with more than 40% missing values
        missing_percentage = data.isnull().mean() * 100
        columns_to_remove = missing_percentage[missing_percentage > 40].index
        if len(columns_to_remove) > 0:
            data.drop(columns=columns_to_remove, inplace=True)
            cleaning_log.append(f"Removed columns: {list(columns_to_remove)} with > 40% missing values")
        
        # Step 2b: Impute missing numeric values with the mean
        for numeric_col in data.select_dtypes(include=['float64', 'int64']).columns:
            if data[numeric_col].isnull().any():
                mean_value = data[numeric_col].mean()
                data[numeric_col].fillna(mean_value, inplace=True)
                cleaning_log.append(f"Imputed missing values in {numeric_col} with mean {mean_value:.2f}")

        # Step 2c: Impute missing categorical values with the mode
        for categorical_col in data.select_dtypes(include='category').columns:
            if data[categorical_col].isnull().any():
                mode_value = data[categorical_col].mode()[0]
                data[categorical_col].fillna(mode_value, inplace=True)
                cleaning_log.append(f"Imputed missing values in {categorical_col} with mode '{mode_value}'")
        
        # STEP 3: Remove Duplicates
        print("üóëÔ∏è Step 3: Removing duplicates...")
        duplicates_before = data.duplicated().sum()
        if duplicates_before > 0:
            data = data.drop_duplicates()
            cleaning_log.append(f"Removed {duplicates_before} duplicate rows")
        
        # STEP 4: Handle Outliers (CONSERVATIVE)
        print("üìà Step 4: Conservative outlier handling...")
        # Example implementation, more could be added based on domain knowledge as needed.
        # Only commented as outlier handling is conservative and not always done.
        
        # STEP 5: Final Validation and Cleanup
        print("‚úÖ Step 5: Final validation...")
        # Remove any remaining rows that are completely empty
        empty_rows = data.isnull().all(axis=1).sum()
        if empty_rows > 0:
            data = data.dropna(how='all')
            cleaning_log.append(f"Removed {empty_rows} completely empty rows")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Error during cleaning: {str(e)}")
        print("üîÑ Returning original data to prevent data loss")
        return data_raw.copy()
    
    # MANDATORY: Data preservation validation
    final_shape = data.shape
    final_rows = len(data)
    data_loss_pct = ((original_rows - final_rows) / original_rows) * 100 if original_rows > 0 else 0
    
    # Comprehensive reporting
    print(f"\nüìà CLEANING SUMMARY:")
    print(f"   Original: {original_shape[0]} rows √ó {original_shape[1]} columns")
    print(f"   Final: {final_shape[0]} rows √ó {final_shape[1]} columns")
    print(f"   Data retention: {100-data_loss_pct:.1f}%")
    
    if cleaning_log:
        print(f"\nüìù Actions taken:")
        for action in cleaning_log:
            print(f"   ‚Ä¢ {action}")
    
    # Critical validation checks
    if data_loss_pct > 25:
        print(f"üö® CRITICAL WARNING: High data loss ({data_loss_pct:.1f}%)!")
    
    if final_rows == 0:
        print(f"‚ùå FATAL ERROR: All data was removed! Returning original data")
        return data_raw.copy()
    
    if final_rows < 10 and original_rows > 100:
        print(f"‚ö†Ô∏è WARNING: Extreme data reduction ({final_rows} remaining from {original_rows})")
    
    # Reset pandas options
    pd.set_option('mode.chained_assignment', 'warn')
    warnings.resetwarnings()
    
    print(f"‚úÖ Data cleaning completed successfully!")
    return data