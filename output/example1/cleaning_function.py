# ----------------------------------------------------------------------
# Code generated by AI agent: data_cleaning_agent
# ----------------------------------------------------------------------

def data_cleaner(data_raw):
    import pandas as pd
    import numpy as np
    from scipy import stats
    import warnings
    
    # Suppress pandas warnings for clean output
    warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)
    pd.set_option('mode.chained_assignment', None)
    
    # ALWAYS work on a copy to preserve original
    data = data_raw.copy()
    
    # Initialize tracking variables
    original_shape = data.shape
    original_rows = len(data)
    cleaning_log = []
    
    print(f"üßπ Starting data cleaning: {original_shape[0]} rows √ó {original_shape[1]} columns")
    
    try:
        # STEP 1: Remove Duplicate Rows
        print("üóëÔ∏è Step 1: Removing duplicates...")
        duplicates_before = data.duplicated().sum()
        if duplicates_before > 0:
            data = data.drop_duplicates()
            cleaning_log.append(f"Removed {duplicates_before} duplicate rows")
        
        # STEP 2: Impute Missing Values
        print("üîß Step 2: Handling missing values...")
        missing_before = data.isnull().sum().sum()
        print(f"   Missing values before cleaning: {missing_before}")
        
        for col in data.columns:
            if data[col].isnull().sum() > 0:
                col_dtype = str(data[col].dtype)
                missing_count = data[col].isnull().sum()
                print(f"   Handling {missing_count} missing values in '{col}' ({col_dtype})")
                
                # Impute missing numeric values with median
                if pd.api.types.is_numeric_dtype(data[col]):
                    fill_value = data[col].median()
                    data[col].fillna(fill_value, inplace=True)
                    cleaning_log.append(f"Filled {missing_count} missing values in '{col}' with median ({fill_value})")
                
                # Impute missing categorical values with 'Unknown'
                elif pd.api.types.is_object_dtype(data[col]) or pd.api.types.is_categorical_dtype(data[col]):
                    mode_value = data[col].mode()[0] if not data[col].mode().empty else 'Unknown'
                    data[col].fillna(mode_value, inplace=True)
                    cleaning_log.append(f"Filled {missing_count} missing values in '{col}' with mode/Unknown ({mode_value})")
        
        missing_after = data.isnull().sum().sum()
        print(f"   Missing values after cleaning: {missing_after}")
        if missing_after < missing_before:
            print(f"   Successfully reduced missing values by {missing_before - missing_after}")
        elif missing_after > 0:
            print(f"   ‚ö†Ô∏è Warning: {missing_after} missing values remain")
        
        # STEP 3: Remove Outliers (if necessary)
        print("üìà Step 3: Assessing outliers (conservative approach)...")
        numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
        for col in numeric_cols:
            Q1 = data[col].quantile(0.25)
            Q3 = data[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outliers_before = data[(data[col] < lower_bound) | (data[col] > upper_bound)].shape[0]
            data.loc[(data[col] < lower_bound) | (data[col] > upper_bound), col] = np.nan  # Mark as NaN
            
            # Impute again after identifying outliers
            if outliers_before > 0:
                fill_value = data[col].median()  # Impute outliers with median
                data[col].fillna(fill_value, inplace=True)
                cleaning_log.append(f"Filled {outliers_before} outliers in '{col}' with median ({fill_value})")
        
        # STEP 4: Final Validation and Cleanup
        print("‚úÖ Step 4: Final validation...")
        empty_rows = data.isnull().all(axis=1).sum()
        if empty_rows > 0:
            data = data.dropna(how='all')
            cleaning_log.append(f"Removed {empty_rows} completely empty rows")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Error during cleaning: {str(e)}")
        print("üîÑ Returning original data to prevent data loss")
        return data_raw.copy()
    
    # Data preservation validation
    final_shape = data.shape
    final_rows = len(data)
    data_loss_pct = ((original_rows - final_rows) / original_rows) * 100 if original_rows > 0 else 0
    
    # Comprehensive reporting
    print(f"\nüìà CLEANING SUMMARY:")
    print(f"   Original: {original_shape[0]} rows √ó {original_shape[1]} columns")
    print(f"   Final: {final_shape[0]} rows √ó {final_shape[1]} columns")
    print(f"   Data retention: {100-data_loss_pct:.1f}%")
    
    if cleaning_log:
        print(f"\nüìù Actions taken:")
        for action in cleaning_log:
            print(f"   ‚Ä¢ {action}")
    
    if data_loss_pct > 20:
        print(f"üö® CRITICAL WARNING: High data loss ({data_loss_pct:.1f}%)!")
    
    if final_rows == 0:
        print(f"‚ùå FATAL ERROR: All data was removed! Returning original data")
        return data_raw.copy()
    
    if final_rows < 10 and original_rows > 100:
        print(f"‚ö†Ô∏è WARNING: Extreme data reduction ({final_rows} remaining from {original_rows})")
    
    # Reset pandas options
    pd.set_option('mode.chained_assignment', 'warn')
    warnings.resetwarnings()
    
    print(f"‚úÖ Data cleaning completed successfully!")
    return data