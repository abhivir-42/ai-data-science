#!/usr/bin/env python3
"""
Data Analysis uAgent Implementation

Following EXACTLY the Fetch.ai LangGraph adapter example:
https://innovationlab.fetch.ai/resources/docs/examples/adapters/langgraph-adapter-example

This wraps the DataAnalysisAgent as a uAgent for deployment on ASI:One.
The wrapper is minimal and leverages the full intelligence of DataAnalysisAgent.
"""

import os
import time
import sys
import re
import pandas as pd
import requests
from dotenv import load_dotenv

# Add src to path
sys.path.append('src')

from uagents_adapter import LangchainRegisterTool, cleanup_uagent
from src.agents.data_analysis_agent import DataAnalysisAgent

# Load environment variables
load_dotenv()

# Set API keys (exactly like the example)
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
API_TOKEN = os.environ.get("AGENTVERSE_API_TOKEN")

if not OPENAI_API_KEY:
    raise ValueError("Please set OPENAI_API_KEY environment variable")

if not API_TOKEN:
    print("Warning: AGENTVERSE_API_TOKEN not set - will register locally only")

# Initialize the enhanced data analysis agent
data_analysis_agent = DataAnalysisAgent(
    output_dir="output/data_analysis_uagent/",
    intent_parser_model="gpt-4o-mini",
    enable_async=False  # Keep synchronous for uAgent stability, but optimized with multi-threading
)

# Global variable to store the last processed data for follow-up requests
_last_cleaned_data = None
_last_processed_timestamp = None

def upload_csv_to_remote_host(file_path, file_description="Processed Data"):
    """
    Upload CSV file to a remote hosting service and return public URL.
    
    Parameters
    ----------
    file_path : str
        Local path to the CSV file
    file_description : str
        Description of the file for naming
        
    Returns
    -------
    dict
        Dictionary containing success status, URL, and metadata
    """
    try:
        if not os.path.exists(file_path):
            return {
                "success": False,
                "error": f"File not found: {file_path}",
                "url": None
            }
        
        # Get file size
        file_size = os.path.getsize(file_path)
        file_size_mb = file_size / (1024 * 1024)
        
        # Check if file is too large (most free services have limits)
        if file_size_mb > 50:  # 50MB limit
            return {
                "success": False,
                "error": f"File too large ({file_size_mb:.1f} MB). Maximum size is 50MB.",
                "url": None
            }
        
        # Try multiple hosting services for reliability
        hosting_services = [
            {
                "name": "tmpfiles.org",
                "url": "https://tmpfiles.org/api/v1/upload",
                "method": "POST"
            }
        ]
        
        for service in hosting_services:
            try:
                print(f"üîÑ Uploading to {service['name']}...")
                
                with open(file_path, 'rb') as file:
                    if service['name'] == "tmpfiles.org":
                        response = requests.post(
                            service['url'],
                            files={'file': file},
                            timeout=30
                        )
                    
                    if response.status_code == 200:
                        result = response.json()
                        
                        # Extract URL based on service
                        if service['name'] == "tmpfiles.org" and result.get('status') == 'success':
                            file_url = result['data']['url']
                            # Convert http to https for better security
                            if file_url.startswith('http://'):
                                file_url = file_url.replace('http://', 'https://')
                            
                            return {
                                "success": True,
                                "url": file_url,
                                "service": service['name'],
                                "file_id": file_url.split('/')[-2] if '/' in file_url else 'unknown',
                                "size_mb": file_size_mb,
                                "error": None,
                                "expires": "60 minutes (auto-delete)"
                            }
                
            except Exception as e:
                print(f"‚ùå Failed to upload to {service['name']}: {str(e)}")
                continue
        
        # If all services failed
        return {
            "success": False,
            "error": "All hosting services failed. File saved locally only.",
            "url": None
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"Upload error: {str(e)}",
            "url": None
        }

def create_shareable_csv_link(file_path, file_name, file_description="Processed Data"):
    """
    Create a shareable link for a CSV file by uploading it to a remote host.
    
    Parameters
    ----------
    file_path : str
        Local path to the CSV file
    file_name : str
        Name of the file for display
    file_description : str
        Description of the file
        
    Returns
    -------
    list
        List of formatted strings for display
    """
    lines = []
    
    try:
        # Get file info
        file_size = os.path.getsize(file_path)
        file_size_kb = file_size / 1024
        file_size_mb = file_size / (1024 * 1024)
        
        # Read CSV to get basic stats
        df = pd.read_csv(file_path)
        
        lines.extend([
            f"üîó **{file_name.replace('_', ' ').title()}** (CSV File - {file_size_kb:.1f} KB):",
            f"   üìä Dataset: {len(df):,} rows √ó {len(df.columns)} columns",
            f"   üìÖ Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}",
            ""
        ])
        
        # Upload to remote host
        upload_result = upload_csv_to_remote_host(file_path, file_description)
        
        if upload_result["success"]:
            lines.extend([
                "üåê **SHAREABLE LINK CREATED**:",
                f"   üîó **Download URL**: {upload_result['url']}",
                f"   üè¢ **Service**: {upload_result['service']}",
                f"   üì¶ **File ID**: {upload_result['file_id']}",
                f"   üìä **Size**: {upload_result['size_mb']:.2f} MB",
                f"   ‚è∞ **Expires**: {upload_result['expires']}",
                "",
                "üí° **How to use**:",
                "   1. Click the URL above to download your processed data",
                "   2. Save the file with a .csv extension",
                "   3. Open in Excel, Python, R, or any data analysis tool",
                "   4. Share the link with colleagues or save for later use",
                "",
                "‚ö†Ô∏è  **Important**: File auto-deletes after 60 minutes. Download promptly!"
            ])
        else:
            # Fallback: Provide local file info and sample data
            lines.extend([
                "‚ùå **REMOTE HOSTING FAILED**:",
                f"   Error: {upload_result['error']}",
                "",
                "üìã **FALLBACK: CSV DATA PREVIEW**:",
                ""
            ])
            
            # Show preview of the data
            if file_size_kb < 100:  # Small file - show more data
                lines.extend([
                    f"üìä **Complete CSV Data** ({len(df):,} rows √ó {len(df.columns)} columns):",
                    "```csv",
                    df.to_csv(index=False),
                    "```",
                    "",
                    "üí° **Usage**: Copy the CSV content above and save as .csv file"
                ])
            else:  # Large file - show preview
                lines.extend([
                    f"üìä **CSV Preview** (First 10 of {len(df):,} rows √ó {len(df.columns)} columns):",
                    "```csv",
                    df.head(10).to_csv(index=False),
                    "```",
                    "",
                    f"üìÅ **Local file**: {file_path}",
                    "üí° **To get complete data**: Ask 'Send my cleaned data in chunks'"
                ])
        
    except Exception as e:
        lines.extend([
            f"‚ùå **Error processing file**: {str(e)}",
            f"üìÅ **Local file location**: {file_path}"
        ])
    
    return lines

def data_analysis_agent_func(query):
    """
    Enhanced data analysis agent function following the LangGraph adapter pattern.
    
    This wrapper:
    - Handles input format conversion (exactly like LangGraph example)
    - Directly invokes DataAnalysisAgent.analyze_from_text()
    - Returns formatted results with actual cleaned data samples
    - Handles follow-up requests for data delivery (chunks, subsets, etc.)
    - Leverages all DataAnalysisAgent intelligence without duplication
    
    The DataAnalysisAgent intelligently:
    - Extracts CSV URLs from text using LLM structured outputs
    - Parses workflow intent to determine which agents to run
    - Executes only the needed agents (cleaning, feature engineering, ML)
    - Returns comprehensive structured results
    """
    global _last_cleaned_data, _last_processed_timestamp
    
    # Handle input if it's a dict with 'input' key (EXACT pattern from example)
    if isinstance(query, dict) and 'input' in query:
        query = query['input']
    
    query_lower = query.lower()
    
    # Handle follow-up data delivery requests
    if any(phrase in query_lower for phrase in [
        'send my data', 'provide my cleaned data', 'show me my processed data',
        'my cleaned dataset', 'give me my data', 'deliver my data',
        'send rows', 'send columns', 'data in chunks', 'split my data'
    ]):
        return handle_data_delivery_request(query)
    
    try:
        # Direct invocation of the underlying DataAnalysisAgent
        # This uses LLM structured outputs to extract CSV URLs and parse intent
        result = data_analysis_agent.analyze_from_text(query)
        
        # Store cleaned data for potential follow-up requests
        try:
            if hasattr(data_analysis_agent, 'data_cleaning_agent') and data_analysis_agent.data_cleaning_agent:
                cleaned_df = data_analysis_agent.data_cleaning_agent.get_data_cleaned()
                if cleaned_df is not None and len(cleaned_df) > 0:
                    _last_cleaned_data = cleaned_df
                    _last_processed_timestamp = time.time()
                    
                    # Add sample data to result for better display
                    sample_rows = cleaned_df.head(3).to_string()
                    result.key_insights.insert(0, f"Sample cleaned data (first 3 rows):\n{sample_rows}")
                    result.key_insights.insert(0, f"Cleaned dataset contains {len(cleaned_df):,} rows and {len(cleaned_df.columns)} columns")
                    
                    # Add column information
                    col_info = []
                    for col in cleaned_df.columns[:10]:  # First 10 columns
                        dtype = cleaned_df[col].dtype
                        null_count = cleaned_df[col].isnull().sum()
                        col_info.append(f"{col}: {dtype} ({null_count} nulls)")
                    
                    if col_info:
                        result.key_insights.insert(0, f"Column details: {', '.join(col_info)}")
                        
        except Exception as e:
            # Silent fail - don't break the main functionality
            print(f"Could not extract sample data: {str(e)}")
        
        # Format the structured result for uAgent compatibility
        return format_analysis_result(result)
        
    except Exception as e:
        error_msg = f"""
üö´ **Analysis Error**

Sorry, I encountered an issue: {str(e)}

**Common solutions:**
1. Include a direct CSV URL in your request (e.g., https://example.com/data.csv)
2. Be specific about what analysis you want
3. Example: "Clean and analyze https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv for survival prediction"

**Need help?** Try: "Analyze https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv for species classification"
"""
        print(f"‚ùå Error in data analysis agent: {str(e)}")
        return error_msg

def handle_data_delivery_request(query):
    """Handle follow-up requests for data delivery in various formats."""
    global _last_cleaned_data, _last_processed_timestamp
    
    # Check if we have recent cleaned data
    if _last_cleaned_data is None:
        return """
üö´ **No Recent Data Found**

I don't have any recently processed data to deliver. Please first run a data cleaning task, for example:

"Clean and analyze https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv"

Then I can provide your cleaned data in various formats.
"""
    
    # Check if data is too old (older than 1 hour)
    if _last_processed_timestamp and (time.time() - _last_processed_timestamp) > 3600:
        return """
üïê **Data Session Expired**

Your cleaned data session has expired (older than 1 hour). Please re-run your data cleaning task to get fresh results.
"""
    
    try:
        df = _last_cleaned_data
        query_lower = query.lower()
        
        # Parse the request type
        if 'chunk' in query_lower:
            # Extract number of chunks if specified
            chunk_match = re.search(r'(\d+)\s*chunk', query_lower)
            num_chunks = int(chunk_match.group(1)) if chunk_match else 5
            return deliver_data_in_chunks(df, num_chunks)
        
        elif 'rows' in query_lower:
            # Extract row range if specified
            range_match = re.search(r'rows?\s*(\d+)[-\s]*(\d+)?', query_lower)
            if range_match:
                start_row = int(range_match.group(1)) - 1  # Convert to 0-indexed
                end_row = int(range_match.group(2)) if range_match.group(2) else start_row + 1000
                return deliver_data_rows(df, start_row, end_row)
            else:
                return deliver_data_rows(df, 0, min(1000, len(df)))
        
        elif 'column' in query_lower:
            # Extract column range or names if specified
            col_match = re.search(r'columns?\s*(\d+)[-\s]*(\d+)?', query_lower)
            if col_match:
                start_col = int(col_match.group(1)) - 1  # Convert to 0-indexed
                end_col = int(col_match.group(2)) if col_match.group(2) else start_col + 5
                return deliver_data_columns(df, start_col, end_col)
            else:
                return deliver_data_columns(df, 0, min(5, len(df.columns)))
        
        else:
            # Default: provide complete data if small enough, otherwise chunked
            csv_content = df.to_csv(index=False)
            content_size = len(csv_content.encode('utf-8'))
            
            if content_size < 100000:  # 100KB limit for direct delivery
                return f"""
üìÅ **YOUR COMPLETE CLEANED DATA**

File size: {content_size / 1024:.1f} KB | Rows: {len(df):,} | Columns: {len(df.columns)}

```csv
{csv_content}
```

üí° **Usage**: Copy the CSV content above and save it as a .csv file for use in Excel, Python, R, or other tools.
"""
            else:
                return deliver_data_in_chunks(df, 5)
    
    except Exception as e:
        return f"""
üö´ **Data Delivery Error**

Sorry, I encountered an issue delivering your data: {str(e)}

Please try a more specific request like:
- "Send me rows 1-100 of my data"
- "Provide my data in 3 chunks"
- "Show me columns 1-5 of my cleaned data"
"""

def deliver_data_in_chunks(df, num_chunks):
    """Deliver data in specified number of chunks."""
    chunk_size = len(df) // num_chunks
    if chunk_size == 0:
        chunk_size = 1
    
    result = [f"""
üì¶ **CHUNKED DATA DELIVERY**

Your cleaned data ({len(df):,} rows √ó {len(df.columns)} columns) split into {num_chunks} chunks.
Each chunk contains approximately {chunk_size} rows.

"""]
    
    for i in range(num_chunks):
        start_idx = i * chunk_size
        end_idx = min((i + 1) * chunk_size, len(df)) if i < num_chunks - 1 else len(df)
        chunk_df = df.iloc[start_idx:end_idx]
        
        result.append(f"""
üìã **CHUNK {i+1}/{num_chunks}** (Rows {start_idx+1}-{end_idx})

```csv
{chunk_df.to_csv(index=False)}
```
""")
    
    result.append("""
üí° **Combine chunks**: Copy each chunk and concatenate them to reconstruct your complete dataset.
""")
    
    return "\n".join(result)

def deliver_data_rows(df, start_row, end_row):
    """Deliver specific row range."""
    end_row = min(end_row, len(df))
    subset_df = df.iloc[start_row:end_row]
    
    return f"""
üìã **DATA ROWS {start_row+1}-{end_row}**

Showing {len(subset_df)} rows from your cleaned dataset:

```csv
{subset_df.to_csv(index=False)}
```

üí° **Need more rows?** Ask: "Send me rows {end_row+1}-{min(end_row+1000, len(df))}" for the next batch.
Total dataset has {len(df):,} rows.
"""

def deliver_data_columns(df, start_col, end_col):
    """Deliver specific column range."""
    end_col = min(end_col, len(df.columns))
    subset_df = df.iloc[:, start_col:end_col]
    
    selected_cols = df.columns[start_col:end_col].tolist()
    
    return f"""
üìã **DATA COLUMNS {start_col+1}-{end_col}**

Showing columns: {', '.join(selected_cols)}

```csv
{subset_df.to_csv(index=False)}
```

üí° **Need more columns?** Ask: "Send me columns {end_col+1}-{min(end_col+5, len(df.columns))}" for the next set.
Total dataset has {len(df.columns)} columns: {', '.join(df.columns.tolist())}
"""

def display_file_contents(file_name, file_path):
    """Display the contents of a generated file based on its type."""
    file_lines = []
    
    try:
        if not os.path.exists(file_path):
            file_lines.extend([
                f"üìÑ **{file_name.replace('_', ' ').title()}**:",
                f"   ‚ö†Ô∏è File not found: {file_path}",
                ""
            ])
            return file_lines
        
        # Get file size
        file_size = os.path.getsize(file_path)
        file_size_kb = file_size / 1024
        
        # Determine file type and display strategy
        file_ext = os.path.splitext(file_path)[1].lower()
        
        if file_ext == '.csv':
            # Handle CSV files with remote hosting
            file_lines.extend(create_shareable_csv_link(file_path, file_name, "Processed CSV Data"))
        
        elif file_ext == '.txt' or file_ext == '.log':
            # Handle text/log files
            file_lines.extend([
                f"üìù **{file_name.replace('_', ' ').title()}** (Text File - {file_size_kb:.1f} KB):",
                ""
            ])
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                if len(content) < 10000:  # Small text file - show full content
                    file_lines.extend([
                        "```",
                        content,
                        "```"
                    ])
                else:  # Large text file - show first part
                    preview_content = content[:5000] + "\n\n... (truncated for display) ..."
                    file_lines.extend([
                        f"üìÑ **Content Preview** (First 5000 characters of {len(content):,} total):",
                        "```",
                        preview_content,
                        "```",
                        "",
                        f"üìÅ **Full file location**: {file_path}"
                    ])
            
            except Exception as e:
                file_lines.extend([
                    f"‚ö†Ô∏è Could not read text file: {str(e)}",
                    f"üìÅ File location: {file_path}"
                ])
        
        elif file_ext in ['.py', '.r', '.sql']:
            # Handle code files
            file_lines.extend([
                f"üíª **{file_name.replace('_', ' ').title()}** (Code File - {file_size_kb:.1f} KB):",
                ""
            ])
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Determine language for syntax highlighting
                lang_map = {'.py': 'python', '.r': 'r', '.sql': 'sql'}
                lang = lang_map.get(file_ext, 'text')
                
                if len(content) < 8000:  # Show full code file
                    file_lines.extend([
                        f"```{lang}",
                        content,
                        "```"
                    ])
                else:  # Show preview of large code file
                    preview_content = content[:4000] + "\n\n# ... (truncated for display) ..."
                    file_lines.extend([
                        f"üìÑ **Code Preview** (First 4000 characters):",
                        f"```{lang}",
                        preview_content,
                        "```",
                        "",
                        f"üìÅ **Full file location**: {file_path}"
                    ])
            
            except Exception as e:
                file_lines.extend([
                    f"‚ö†Ô∏è Could not read code file: {str(e)}",
                    f"üìÅ File location: {file_path}"
                ])
        
        else:
            # Handle other file types
            file_lines.extend([
                f"üìÑ **{file_name.replace('_', ' ').title()}** ({file_ext.upper()} File - {file_size_kb:.1f} KB):",
                f"üìÅ File location: {file_path}",
                ""
            ])
            
            # Try to read as text if it's small
            if file_size_kb < 20:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    file_lines.extend([
                        "üìÑ **File Contents**:",
                        "```",
                        content,
                        "```"
                    ])
                except:
                    file_lines.append("‚ö†Ô∏è Binary file - cannot display content as text")
            else:
                file_lines.append("üìÅ File too large to display - check file location above")
    
    except Exception as e:
        file_lines.extend([
            f"üìÑ **{file_name.replace('_', ' ').title()}**:",
            f"   ‚ùå Error reading file: {str(e)}",
            f"   üìÅ File path: {file_path}",
            ""
        ])
    
    return file_lines


def format_analysis_result(result) -> str:
    """Format the analysis result into a comprehensive, user-friendly response."""
    
    try:
        lines = [
            "üéâ **DATA ANALYSIS COMPLETE**",
            "=" * 60,
            "",
            f"üìä **Dataset**: {result.csv_url}",
            f"üìù **Request**: {result.original_request[:200]}{'...' if len(result.original_request) > 200 else ''}",
            f"‚è±Ô∏è  **Runtime**: {result.total_runtime_seconds:.2f} seconds",
            f"üéØ **Confidence**: {result.confidence_level.upper()}",
            f"‚≠ê **Quality Score**: {result.analysis_quality_score:.2f}/1.0",
            ""
        ]
        
        # CHECK FOR AGENT FAILURES FIRST
        failed_agents = []
        successful_agents = []
        
        for agent_result in result.agent_results:
            if not agent_result.success:
                failed_agents.append(agent_result)
            else:
                successful_agents.append(agent_result)
        
        # Report any failures upfront
        if failed_agents:
            lines.extend([
                "‚ö†Ô∏è  **AGENT EXECUTION STATUS**:",
                ""
            ])
            
            for failed_agent in failed_agents:
                error_msg = getattr(failed_agent, 'error_message', 'Unknown error')
                # Handle None or empty error messages
                if not error_msg or error_msg.strip() == '':
                    error_msg = f"{failed_agent.agent_name} execution failed without specific error details"
                
                lines.extend([
                    f"   ‚ùå **{failed_agent.agent_name.replace('_', ' ').title()} Agent**: FAILED",
                    f"       Error: {error_msg}",
                    f"       Runtime: {failed_agent.execution_time_seconds:.2f}s",
                    ""
                ])
            
            for successful_agent in successful_agents:
                lines.extend([
                    f"   ‚úÖ **{successful_agent.agent_name.replace('_', ' ').title()} Agent**: SUCCESS",
                    f"       Runtime: {successful_agent.execution_time_seconds:.2f}s",
                    ""
                ])
            
            lines.extend([
                "üìä **ANALYSIS CONTINUES WITH AVAILABLE DATA**:",
                "   Even with some agent failures, we'll provide results from successful steps.",
                ""
            ])
        else:
            lines.extend([
                "‚úÖ **AGENT EXECUTION STATUS**:",
                ""
            ])
            
            for successful_agent in successful_agents:
                lines.extend([
                    f"   ‚úÖ **{successful_agent.agent_name.replace('_', ' ').title()} Agent**: SUCCESS",
                    f"       Runtime: {successful_agent.execution_time_seconds:.2f}s",
                    ""
                ])
            
            lines.extend([
                "üéâ **ALL AGENTS EXECUTED SUCCESSFULLY**:",
                ""
            ])
        
        # SHOW ACTUAL DATA TRANSFORMATION RESULTS
        lines.extend([
            "üìà **DATA TRANSFORMATION RESULTS**:",
            ""
        ])
        
        # Original vs Final data shape
        original_rows = result.data_shape.get('rows', 'unknown')
        original_cols = result.data_shape.get('columns', 'unknown')
        
        # Extract final shape from agent results if available
        final_rows = original_rows
        final_cols = original_cols
        data_retention = 100.0
        
        # Parse agent results for actual cleaning metrics
        for agent_result in result.agent_results:
            if agent_result.agent_name == "data_cleaning" and agent_result.log_messages:
                log_text = " ".join(agent_result.log_messages)
                # Extract actual numbers from logs
                
                # Look for "Original: X rows √ó Y columns"
                original_match = re.search(r'Original:\s*(\d+)\s*rows\s*√ó\s*(\d+)\s*columns', log_text)
                if original_match:
                    original_rows = int(original_match.group(1))
                    original_cols = int(original_match.group(2))
                
                # Look for "Final: X rows √ó Y columns"
                final_match = re.search(r'Final:\s*(\d+)\s*rows\s*√ó\s*(\d+)\s*columns', log_text)
                if final_match:
                    final_rows = int(final_match.group(1))
                    final_cols = int(final_match.group(2))
                
                # Look for "Data retention: X%"
                retention_match = re.search(r'Data retention:\s*([\d.]+)%', log_text)
                if retention_match:
                    data_retention = float(retention_match.group(1))
        
        lines.extend([
            f"   üìè **Before**: {original_rows:,} rows √ó {original_cols} columns",
            f"   ‚ú® **After**: {final_rows:,} rows √ó {final_cols} columns",
            f"   üìä **Data Retention**: {data_retention:.1f}%",
            f"   üîÑ **Rows Changed**: {final_rows - original_rows:+,}" if isinstance(final_rows, int) and isinstance(original_rows, int) else "",
            ""
        ])
        
        # DETAILED CLEANING ACTIONS (extract from logs)
        cleaning_actions = []
        missing_handled = 0
        outliers_removed = 0
        duplicates_removed = 0
        columns_dropped = []
        
        for agent_result in result.agent_results:
            if agent_result.agent_name == "data_cleaning" and agent_result.log_messages:
                log_text = " ".join(agent_result.log_messages)
                
                # Extract specific actions
                missing_matches = re.findall(r'Filled (\d+) missing values in [\'"]([^\'"]+)[\'"] with (\w+)', log_text)
                for count, column, method in missing_matches:
                    missing_handled += int(count)
                    cleaning_actions.append(f"Filled {count} missing values in '{column}' with {method}")
                
                # Extract outlier information
                outlier_matches = re.findall(r'Removed (\d+) outliers from [\'"]([^\'"]+)[\'"]', log_text)
                for count, column in outlier_matches:
                    outliers_removed += int(count)
                    cleaning_actions.append(f"Removed {count} outliers from '{column}'")
                
                # Extract dropped columns
                dropped_matches = re.findall(r'Dropped [\'"]([^\'"]+)[\'"] column', log_text)
                columns_dropped.extend(dropped_matches)
                for column in dropped_matches:
                    cleaning_actions.append(f"Dropped '{column}' column due to high missing values")
        
        if cleaning_actions:
            lines.extend([
                "üßπ **CLEANING ACTIONS PERFORMED**:",
                *[f"   ‚Ä¢ {action}" for action in cleaning_actions[:10]],  # Limit to 10 actions
                f"   ‚Ä¢ ...and {len(cleaning_actions) - 10} more actions" if len(cleaning_actions) > 10 else "",
                ""
            ])
        
        # DATA QUALITY IMPROVEMENTS
        if missing_handled > 0 or outliers_removed > 0:
            lines.extend([
                "üìä **DATA QUALITY IMPROVEMENTS**:",
                f"   ‚Ä¢ Missing values handled: {missing_handled:,}",
                f"   ‚Ä¢ Outliers removed: {outliers_removed:,}",
                f"   ‚Ä¢ Columns dropped: {len(columns_dropped)}",
                ""
            ])
        
        # PROVIDE ACTUAL CLEANED DATA TO USER
        # Try to read the cleaned data file and provide it to the user
        cleaned_data_provided = False
        
        # Look for cleaned data file path in generated files
        cleaned_data_path = None
        for agent_result in result.agent_results:
            if agent_result.output_data_path and agent_result.agent_name == "data_cleaning":
                cleaned_data_path = agent_result.output_data_path
                break
        
        if cleaned_data_path:
            try:
                # Try to read the cleaned data
                if os.path.exists(cleaned_data_path):
                    cleaned_df = pd.read_csv(cleaned_data_path)
                    
                    # Calculate size to determine delivery method
                    csv_content = cleaned_df.to_csv(index=False)
                    content_size = len(csv_content.encode('utf-8'))
                    
                    lines.extend([
                        "üìä **CLEANED DATA STATISTICS**:",
                        f"   ‚Ä¢ Total rows: {len(cleaned_df):,}",
                        f"   ‚Ä¢ Total columns: {len(cleaned_df.columns)}",
                        f"   ‚Ä¢ Missing values: {cleaned_df.isnull().sum().sum():,}",
                        f"   ‚Ä¢ File size: {content_size / 1024:.1f} KB",
                        ""
                    ])
                    
                    # Strategy 1: For small datasets (< 50KB), provide full CSV content
                    if content_size < 50000:  # 50KB limit
                        lines.extend([
                            "üìÅ **YOUR CLEANED DATA** (Complete CSV):",
                            "```csv",
                            csv_content,
                            "```",
                            "",
                            "üí° **How to use**: Copy the CSV content above and save it as a .csv file, or use it directly in your analysis.",
                            ""
                        ])
                        cleaned_data_provided = True
                    
                    # Strategy 2: For medium datasets (50KB-200KB), provide compressed or chunked data
                    elif content_size < 200000:  # 200KB limit
                        # Provide key columns and sample + instructions
                        lines.extend([
                            "üìã **CLEANED DATA PREVIEW** (First 10 rows):",
                            "```csv",
                            cleaned_df.head(10).to_csv(index=False),
                            "```",
                            "",
                            f"üìÅ **FULL DATASET INFORMATION**:",
                            f"   ‚Ä¢ Dataset is {content_size / 1024:.1f} KB - too large to display fully here",
                            f"   ‚Ä¢ Contains {len(cleaned_df):,} rows and {len(cleaned_df.columns)} columns",
                            "",
                            "üí° **To get your complete cleaned data**:",
                            "   1. Ask: 'Please provide my cleaned data in chunks'",
                            "   2. Or: 'Split my cleaned data into smaller parts'",
                            "   3. I can deliver it in manageable pieces you can combine",
                            ""
                        ])
                        cleaned_data_provided = True
                    
                    # Strategy 3: For large datasets (>200KB), provide summary and delivery options
                    else:
                        lines.extend([
                            "üìã **CLEANED DATA SUMMARY** (First 5 rows):",
                            "```csv",
                            cleaned_df.head(5).to_csv(index=False),
                            "```",
                            "",
                            f"üìÅ **LARGE DATASET DETECTED**:",
                            f"   ‚Ä¢ Dataset is {content_size / 1024:.1f} KB ({content_size / 1024 / 1024:.2f} MB)" if content_size > 1024*1024 else f"   ‚Ä¢ Dataset is {content_size / 1024:.1f} KB",
                            f"   ‚Ä¢ Contains {len(cleaned_df):,} rows and {len(cleaned_df.columns)} columns",
                            "",
                            "üí° **Delivery Options for Your Complete Data**:",
                            "   1. **Chunked Delivery**: Ask 'Send my data in 10 chunks'",
                            "   2. **Column Subsets**: Ask 'Send columns 1-5 of my cleaned data'",
                            "   3. **Row Ranges**: Ask 'Send rows 1-1000 of my cleaned data'",
                            "   4. **Filtered Data**: Ask 'Send only [specific columns/conditions]'",
                            "",
                            "üéØ **Quick Access**: Ask 'How can I download my complete cleaned dataset?'",
                            ""
                        ])
                        cleaned_data_provided = True
                    
                    # Show column information for all cases
                    lines.extend([
                        "üìã **COLUMN INFORMATION**:",
                    ])
                    
                    for col in cleaned_df.columns[:15]:  # Show first 15 columns
                        dtype = cleaned_df[col].dtype
                        null_count = cleaned_df[col].isnull().sum()
                        unique_count = cleaned_df[col].nunique()
                        
                        # Add sample values for categorical columns
                        if dtype == 'object' and unique_count <= 10:
                            sample_values = cleaned_df[col].value_counts().head(3).index.tolist()
                            sample_str = f" (e.g., {', '.join(map(str, sample_values))})"
                        else:
                            sample_str = ""
                        
                        lines.append(f"   ‚Ä¢ {col}: {dtype} | {null_count} nulls | {unique_count} unique{sample_str}")
                    
                    if len(cleaned_df.columns) > 15:
                        lines.append(f"   ‚Ä¢ ...and {len(cleaned_df.columns) - 15} more columns")
                    
                    lines.append("")
                
            except Exception as e:
                lines.extend([
                    "‚ö†Ô∏è  **Note**: Could not access cleaned data file",
                    f"   Error: {str(e)}",
                    "   The data was processed but file access failed",
                    ""
                ])
        
        # Alternative: Try to access the DataAnalysisAgent's cleaned data directly
        if not cleaned_data_provided:
            try:
                # Try to extract cleaned data from agent logs or results
                for agent_result in result.agent_results:
                    if agent_result.agent_name == "data_cleaning" and hasattr(agent_result, 'data_quality_metrics'):
                        if hasattr(agent_result.data_quality_metrics, 'cleaned_shape'):
                            lines.extend([
                                "üìã **CLEANED DATA INFORMATION**:",
                                f"   ‚úÖ Data successfully cleaned and processed",
                                f"   üìä Shape: {agent_result.data_quality_metrics.cleaned_shape.get('rows', 'unknown'):,} rows √ó {agent_result.data_quality_metrics.cleaned_shape.get('columns', 'unknown')} columns" if hasattr(agent_result.data_quality_metrics, 'cleaned_shape') else "",
                                "",
                                "üí° **To Access Your Cleaned Data**:",
                                "   Ask: 'Please provide my cleaned dataset' or 'Show me my processed data'",
                                ""
                            ])
                            cleaned_data_provided = True
                            break
                
            except Exception as e:
                pass  # Silent fail for this fallback attempt
        
        if not cleaned_data_provided:
            lines.extend([
                "üìã **CLEANED DATA**:",
                f"   ‚úÖ Data successfully cleaned and saved",
                f"   üìä Final shape: {final_rows:,} rows √ó {final_cols} columns",
                "",
                "üí° **To Get Your Cleaned Data**:",
                "   Ask: 'Please provide my cleaned dataset as CSV' or 'Send me my processed data'",
                ""
            ])
        
        # Workflow information with actual execution results
        if result.workflow_intent:
            # Check actual execution results
            data_cleaning_status = "‚ùå Not executed"
            feature_engineering_status = "‚ùå Not executed"  
            ml_modeling_status = "‚ùå Not executed"
            
            for agent_result in result.agent_results:
                if agent_result.agent_name == "data_cleaning":
                    data_cleaning_status = "‚úÖ Success" if agent_result.success else f"‚ùå Failed: {getattr(agent_result, 'error_message', 'Unknown error')[:50]}..."
                elif agent_result.agent_name == "feature_engineering":
                    feature_engineering_status = "‚úÖ Success" if agent_result.success else f"‚ùå Failed: {getattr(agent_result, 'error_message', 'Unknown error')[:50]}..."
                elif agent_result.agent_name == "h2o_ml":
                    ml_modeling_status = "‚úÖ Success" if agent_result.success else f"‚ùå Failed: {getattr(agent_result, 'error_message', 'Unknown error')[:50]}..."
            
            lines.extend([
                "üîÑ **WORKFLOW EXECUTION RESULTS**:",
                f"   ‚Ä¢ Data Cleaning: {data_cleaning_status}",
                f"   ‚Ä¢ Feature Engineering: {feature_engineering_status}",
                f"   ‚Ä¢ ML Modeling: {ml_modeling_status}",
                f"   ‚Ä¢ Intent Confidence: {result.workflow_intent.intent_confidence:.2f}",
                ""
            ])
        
        # Agents executed
        if result.agents_executed:
            lines.extend([
                "ü§ñ **AGENTS EXECUTED**:",
                *[f"   ‚Ä¢ {agent.replace('_', ' ').title()}" for agent in result.agents_executed],
                ""
            ])
        
        # Performance metrics
        metrics_added = False
        if result.overall_data_quality_score is not None:
            if not metrics_added:
                lines.extend(["üìà **PERFORMANCE METRICS**:", ""])
                metrics_added = True
            lines.append(f"   ‚Ä¢ Data Quality: {result.overall_data_quality_score:.2f}/1.0")
        
        if hasattr(result, 'feature_engineering_effectiveness') and result.feature_engineering_effectiveness is not None:
            if not metrics_added:
                lines.extend(["üìà **PERFORMANCE METRICS**:", ""])
                metrics_added = True
            lines.append(f"   ‚Ä¢ Feature Engineering: {result.feature_engineering_effectiveness:.2f}/1.0")
        
        if hasattr(result, 'model_performance_score') and result.model_performance_score is not None:
            if not metrics_added:
                lines.extend(["üìà **PERFORMANCE METRICS**:", ""])
                metrics_added = True
            lines.append(f"   ‚Ä¢ Model Performance: {result.model_performance_score:.2f}/1.0")
        
        if metrics_added:
            lines.append("")
        
        # Key insights
        if result.key_insights:
            lines.extend([
                "üí° **KEY INSIGHTS**:",
                *[f"   ‚Ä¢ {insight}" for insight in result.key_insights],
                ""
            ])
        
        # Recommendations
        if result.recommendations:
            lines.extend([
                "üéØ **RECOMMENDATIONS**:",
                *[f"   ‚Ä¢ {rec}" for rec in result.recommendations],
                ""
            ])
        
        # Generated files with actual content
        if result.generated_files:
            lines.extend([
                "üìÅ **GENERATED FILES & CONTENTS**:",
                ""
            ])
            
            for name, path in result.generated_files.items():
                lines.extend(display_file_contents(name, path))
                lines.append("")  # Add spacing between files
        
        # Warnings
        if result.warnings:
            lines.extend([
                "‚ö†Ô∏è  **WARNINGS**:",
                *[f"   ‚Ä¢ {warning}" for warning in result.warnings],
                ""
            ])
        
        # Limitations
        if result.limitations:
            lines.extend([
                "‚ö†Ô∏è  **LIMITATIONS**:",
                *[f"   ‚Ä¢ {limitation}" for limitation in result.limitations],
                ""
            ])
        
        lines.extend([
            "=" * 60,
            "‚úÖ **Analysis completed successfully!**",
            "",
            "üí° **What you got:**",
            f"   ‚Ä¢ Cleaned dataset with {data_retention:.1f}% data retention",
            f"   ‚Ä¢ {missing_handled:,} missing values handled" if missing_handled > 0 else "",
            f"   ‚Ä¢ {outliers_removed:,} outliers removed" if outliers_removed > 0 else "",
            "   ‚Ä¢ Detailed cleaning log and generated code",
            "   ‚Ä¢ Ready-to-use data for further analysis",
        ])
        
        return "\n".join(lines)
        
    except Exception as e:
        return f"‚ùå Error formatting result: {str(e)}\n\nRaw result: {str(result)}"

# Register the DataAnalysisAgent via uAgent (EXACT pattern from LangGraph example)
tool = LangchainRegisterTool()

print("üöÄ Registering enhanced data analysis uAgent...")

agent_info = tool.invoke(
    {
        "agent_obj": data_analysis_agent_func,  # Pass the function
        "name": "AI Data Engineer Agent",
        "port": 8102,
        "description": "ü§ñ AI Data Analysis Chatbot - Send me a CSV URL and analysis request. I'll clean your data, engineer features, and build ML models. Example: 'Clean and analyze https://example.com/data.csv for prediction'",
        "api_token": API_TOKEN,
        "mailbox": True
    }
)

print(f"‚úÖ Registration result: {agent_info}")
print(f"üìä Result type: {type(agent_info)}")

# Extract address info for display
if isinstance(agent_info, dict):
    agent_address = agent_info.get('agent_address', 'Unknown')
    agent_port = agent_info.get('agent_port', '8102')
elif isinstance(agent_info, str):
    # If it's a string, extract from logs
    agent_address = "Check logs above for actual address"
    agent_port = "8102"
else:
    agent_address = "Unknown"
    agent_port = "8102"

# Keep the agent alive (EXACT pattern from example)
if __name__ == "__main__":
    try:
        print("\nüéâ ENHANCED DATA ANALYSIS UAGENT IS RUNNING!")
        print("=" * 60)
        print(f"üîó Agent name: AI Data Engineer Agent")
        print(f"üîó Agent address: {agent_address}")
        print(f"üåê Port: {agent_port}")
        print(f"üéØ Inspector: https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A{agent_port}&address={agent_address}")
        print("\nüìã Usage:")
        print("Send a message with a CSV URL and analysis request:")
        print('- "Clean and analyze https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv for survival prediction"')
        print('- "Perform feature engineering on https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv"')
        print('- "Build ML model using https://example.com/your-data.csv to predict target_column"')
        print("\nüéØ The agent uses AI to:")
        print("‚Ä¢ Extract CSV URLs from your text using LLM structured outputs")
        print("‚Ä¢ Parse your intent to determine which analysis steps to run")
        print("‚Ä¢ Execute only the needed agents (cleaning, feature engineering, ML)")
        print("‚Ä¢ Return comprehensive structured results")
        print("\nPress Ctrl+C to stop...")
        
        while True:
            time.sleep(1)
            
    except KeyboardInterrupt:
        print("\nüõë Shutting down AI Data Engineer Agent...")
        cleanup_uagent("AI Data Engineer Agent")
        print("‚úÖ Agent stopped.") 